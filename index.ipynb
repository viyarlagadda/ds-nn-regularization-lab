{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization Lab\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this lab, we'll gain experience detecting and dealing with a ANN model that is overfitting using various regularization and hyperparameter tuning techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this lab, we'll work with a large dataset of customer complaints to a bank, with the goal of predicting what product the customer is complaining about based on the text of their complaint.  There are 7 different possible products that we can predict, making this a multi-class classification task. \n",
    "\n",
    "\n",
    "#### Preprocessing our Data Set\n",
    "We'll start by preprocessing our dataset by tokenizing the complaints and limiting the number of words we consider to reduce dimensionality. \n",
    "\n",
    "#### Building our Tuning our Model\n",
    "Once we have preprocessed our data set, we'll build a model and explore the various ways that we can reduce overfitting using the following strategies:\n",
    "- Early stopping to minimize the discrepancy between train and test accuracy.\n",
    "- L1 and L2 regularization.\n",
    "- Dropout regularization.\n",
    "- Using more data.\n",
    "\n",
    "\n",
    "**_Let's Get Started!_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Bank Complaints Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import the libraries and take a sample\n",
    "\n",
    "Run the cell below to import everything we'll need for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.42.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.30.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (61.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.1)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.1.1)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.6.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.9)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\viyarlagadda\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.4)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, google-pasta, gast, flatbuffers, astunparse, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 flatbuffers-23.1.21 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 libclang-15.0.6.1 oauthlib-3.2.2 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.30.0 termcolor-2.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the cell below, import our data into a DataFrame.  The data is currently stored in `Bank_complaints.csv`.\n",
    "Then, `.describe()` the dataset to get a feel for what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv(\"Bank_complaints.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>59724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am filing this complaint because Experian ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11404</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Product                       Consumer complaint narrative\n",
       "count          60000                                              60000\n",
       "unique             7                                              59724\n",
       "top     Student loan  I am filing this complaint because Experian ha...\n",
       "freq           11404                                                 26"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed things up during the development process (and also to give us the ability to see how adding more data affects our model performance), we're going to work with a sample of our dataset rather than the whole thing.  The entire dataset consists of 60,000 rows--we're going to build a model using only 10,000 items randomly sampled from this.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Get a random sample of `10000` items from our dataset (HINT: use the `df` object's `.sample()` method to make this easy)\n",
    "* Reset the indexes on these samples to `range(10000)`, so that the indices for our rows are sequential and make sense.\n",
    "* Store our labels, which are found in `\"Product\"`, in a different variable.\n",
    "* Store the data, found in `\"Consumer complaint narrative`, in the variable `complaints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenizing the Complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only keep 2,000 most common words and use one-hot encoding to quickly vectorize our dataset from text into a format that a neural network can work with. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `Tokenizer()` object, and set the `num_words` parameter to `2000`.\n",
    "* Call the tokenizer object's `fit_on_texts()` method and pass in our `complaints` variable we created above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create some text sequences by calling the `tokenizer` object's `.texts_to_sequences()` method and feeding in our `complaints` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll convert our text data from text to a vectorized matrix.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Call the `tokenizer` object's `.texts_to_matrix` method, passing in our `complaints` variable, as well as setting the `mode` parameter equal to `'binary'`.\n",
    "* Store the tokenizer's `.word_index` in the appropriate variable.\n",
    "* Check the `np.shape()` of our `one_hot_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results) # Expected Results (10000, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 One-hot Encoding of the Products Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized and encoded our text data, we still need to one-hot encode our label data.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "\n",
    "* Create a `LabelEncoder` object, which can found inside the `preprocessing` module.\n",
    "* `fit` the label encoder we just created to `product`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what classes our label encoder found.  Run the cell below to examine a list of classes that `product` contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bank account or service',\n",
       " 'Checking or savings account',\n",
       " 'Consumer Loan',\n",
       " 'Credit card',\n",
       " 'Credit reporting',\n",
       " 'Mortgage',\n",
       " 'Student loan']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to transform `product` into a numeric vector.  \n",
    "\n",
    "In the cell below, use the label encoder's `.transform` method on `product` to create an integer encoded version of our labels. \n",
    "\n",
    "Then, access `product_cat` to see an example of how the labels are now encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 1, ..., 3, 6, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cat = le.transform(product)\n",
    "product_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to go from integer encoding to one-hot encoding.  Use the `to_categorical` method from keras to do this easily in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the shape of our one-hot encoded labels to make sure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(product_onehot) # Expected Output: (10000, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train - test split\n",
    "\n",
    "Now, we'll split our data into training and testing sets.  \n",
    "\n",
    "\n",
    "We'll accomplish this by generating a random list of 1500 different indices between 1 and 10000.  Then, we'll slice these rows and store them as our test set, and delete them from the training set (it's very important to remember to remove them from the training set!)\n",
    "\n",
    "Run the cell below to create a set of random indices for our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = random.sample(range(1,10000), 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "\n",
    "* Slice the `test_index` rows from `one_hot_results` and store them in `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = one_hot_results[test_index]\n",
    "\n",
    "# This line returns a version of our one_hot_results that has every item with an index in test_index removed\n",
    "train = np.delete(one_hot_results, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to repeat the splitting process on our labels, making sure that we use the same indices we used to split our data. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Slice `test_index` from `product_onehot`\n",
    "* Use `np.delete` to remove `test_index` items from `product_onehot` (the syntax is exactly the same above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the shape everything we just did to make sure that the dimensions match up.  \n",
    "\n",
    "In the cell below, use `np.shape` to check the shape of:\n",
    "\n",
    "* `label_test`\n",
    "* `label_train`\n",
    "* `test`\n",
    "* `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 7)\n",
      "(8500, 7)\n",
      "(1500, 2000)\n",
      "(8500, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(label_test)) # Expected Output: (1500, 7)\n",
    "print(np.shape(label_train)) # Expected Output: (8500, 7)\n",
    "print(np.shape(test)) # Expected Output: (1500, 2000)\n",
    "print(np.shape(train)) # Expected Output: (8500, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`.\n",
    "\n",
    "Run the cell below to create our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "val = train[:1000]\n",
    "train_final = train[1000:]\n",
    "label_val = label_train[:1000]\n",
    "label_train_final = label_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating, compiling and running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Sequential` from the appropriate module in keras.\n",
    "* Import `Dense` from the appropriate module in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a model with the following specifications in the cell below:\n",
    "\n",
    "* An input layer of shape `(2000,)`\n",
    "* Hidden layer 1: Dense, 50 neurons, relu activation \n",
    "* Hidden layer 2: Dense, 25 neurons, relu activation\n",
    "* Output layer: Dense, 7 neurons, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) \n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, `compile` the model with the following settings:\n",
    "\n",
    "* Optimizer is `\"SGD\"`\n",
    "* Loss is `'categorical_crossentropy'`\n",
    "* metrics is `['accuracy']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"SGD\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Train the model for 120 epochs in mini-batches of 256 samples. Also pass in `(val, label_val)` to the `validation_data` parameter, so that we see how our model does on the test set after every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 2s 19ms/step - loss: 1.9578 - accuracy: 0.1409 - val_loss: 1.9469 - val_accuracy: 0.1650\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.9389 - accuracy: 0.1787 - val_loss: 1.9332 - val_accuracy: 0.1980\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9253 - accuracy: 0.2047 - val_loss: 1.9213 - val_accuracy: 0.2180\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9122 - accuracy: 0.2295 - val_loss: 1.9095 - val_accuracy: 0.2490\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.8981 - accuracy: 0.2519 - val_loss: 1.8968 - val_accuracy: 0.2680\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.8820 - accuracy: 0.2693 - val_loss: 1.8820 - val_accuracy: 0.2840\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.8636 - accuracy: 0.2957 - val_loss: 1.8650 - val_accuracy: 0.3080\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.8422 - accuracy: 0.3172 - val_loss: 1.8433 - val_accuracy: 0.3310\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.8162 - accuracy: 0.3364 - val_loss: 1.8172 - val_accuracy: 0.3380\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.7844 - accuracy: 0.3553 - val_loss: 1.7843 - val_accuracy: 0.3550\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.7465 - accuracy: 0.3759 - val_loss: 1.7467 - val_accuracy: 0.3870\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.7032 - accuracy: 0.4013 - val_loss: 1.7019 - val_accuracy: 0.4060\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.6541 - accuracy: 0.4297 - val_loss: 1.6517 - val_accuracy: 0.4350\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.6002 - accuracy: 0.4617 - val_loss: 1.5975 - val_accuracy: 0.4560\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.5430 - accuracy: 0.4952 - val_loss: 1.5407 - val_accuracy: 0.4920\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.4841 - accuracy: 0.5220 - val_loss: 1.4830 - val_accuracy: 0.5140\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.4244 - accuracy: 0.5456 - val_loss: 1.4248 - val_accuracy: 0.5420\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.3652 - accuracy: 0.5671 - val_loss: 1.3680 - val_accuracy: 0.5550\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.3079 - accuracy: 0.5891 - val_loss: 1.3138 - val_accuracy: 0.5680\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.2534 - accuracy: 0.6109 - val_loss: 1.2602 - val_accuracy: 0.5890\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.2017 - accuracy: 0.6305 - val_loss: 1.2111 - val_accuracy: 0.6090\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1538 - accuracy: 0.6455 - val_loss: 1.1668 - val_accuracy: 0.6160\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.1088 - accuracy: 0.6612 - val_loss: 1.1241 - val_accuracy: 0.6350\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0673 - accuracy: 0.6725 - val_loss: 1.0870 - val_accuracy: 0.6370\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0291 - accuracy: 0.6829 - val_loss: 1.0503 - val_accuracy: 0.6540\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9939 - accuracy: 0.6892 - val_loss: 1.0186 - val_accuracy: 0.6630\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9619 - accuracy: 0.6981 - val_loss: 0.9872 - val_accuracy: 0.6710\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9323 - accuracy: 0.7064 - val_loss: 0.9620 - val_accuracy: 0.6750\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9055 - accuracy: 0.7113 - val_loss: 0.9397 - val_accuracy: 0.6800\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8805 - accuracy: 0.7141 - val_loss: 0.9151 - val_accuracy: 0.6880\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8580 - accuracy: 0.7217 - val_loss: 0.8955 - val_accuracy: 0.6940\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8368 - accuracy: 0.7259 - val_loss: 0.8767 - val_accuracy: 0.7100\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8173 - accuracy: 0.7283 - val_loss: 0.8610 - val_accuracy: 0.7030\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7994 - accuracy: 0.7328 - val_loss: 0.8452 - val_accuracy: 0.7080\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7831 - accuracy: 0.7392 - val_loss: 0.8315 - val_accuracy: 0.7120\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7677 - accuracy: 0.7403 - val_loss: 0.8186 - val_accuracy: 0.7170\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7534 - accuracy: 0.7441 - val_loss: 0.8064 - val_accuracy: 0.7150\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7398 - accuracy: 0.7480 - val_loss: 0.7962 - val_accuracy: 0.7190\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7276 - accuracy: 0.7488 - val_loss: 0.7847 - val_accuracy: 0.7260\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7158 - accuracy: 0.7521 - val_loss: 0.7782 - val_accuracy: 0.7270\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7048 - accuracy: 0.7564 - val_loss: 0.7687 - val_accuracy: 0.7190\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6940 - accuracy: 0.7601 - val_loss: 0.7640 - val_accuracy: 0.7270\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6839 - accuracy: 0.7612 - val_loss: 0.7514 - val_accuracy: 0.7300\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6744 - accuracy: 0.7659 - val_loss: 0.7452 - val_accuracy: 0.7320\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6650 - accuracy: 0.7717 - val_loss: 0.7406 - val_accuracy: 0.7370\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6569 - accuracy: 0.7712 - val_loss: 0.7349 - val_accuracy: 0.7260\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6483 - accuracy: 0.7753 - val_loss: 0.7306 - val_accuracy: 0.7290\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6403 - accuracy: 0.7779 - val_loss: 0.7217 - val_accuracy: 0.7360\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6323 - accuracy: 0.7804 - val_loss: 0.7216 - val_accuracy: 0.7330\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6254 - accuracy: 0.7820 - val_loss: 0.7142 - val_accuracy: 0.7410\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.6184 - accuracy: 0.7827 - val_loss: 0.7095 - val_accuracy: 0.7400\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6113 - accuracy: 0.7871 - val_loss: 0.7068 - val_accuracy: 0.7410\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6049 - accuracy: 0.7888 - val_loss: 0.7022 - val_accuracy: 0.7440\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5980 - accuracy: 0.7912 - val_loss: 0.6989 - val_accuracy: 0.7430\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5923 - accuracy: 0.7915 - val_loss: 0.6974 - val_accuracy: 0.7500\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5859 - accuracy: 0.7948 - val_loss: 0.6910 - val_accuracy: 0.7510\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5795 - accuracy: 0.7973 - val_loss: 0.6868 - val_accuracy: 0.7540\n",
      "Epoch 58/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5741 - accuracy: 0.7973 - val_loss: 0.6863 - val_accuracy: 0.7550\n",
      "Epoch 59/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5684 - accuracy: 0.7999 - val_loss: 0.6820 - val_accuracy: 0.7520\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5635 - accuracy: 0.8029 - val_loss: 0.6800 - val_accuracy: 0.7560\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5576 - accuracy: 0.8039 - val_loss: 0.6794 - val_accuracy: 0.7510\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5520 - accuracy: 0.8099 - val_loss: 0.6747 - val_accuracy: 0.7550\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5476 - accuracy: 0.8083 - val_loss: 0.6733 - val_accuracy: 0.7560\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5425 - accuracy: 0.8097 - val_loss: 0.6733 - val_accuracy: 0.7560\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5376 - accuracy: 0.8145 - val_loss: 0.6703 - val_accuracy: 0.7580\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5325 - accuracy: 0.8140 - val_loss: 0.6682 - val_accuracy: 0.7520\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5278 - accuracy: 0.8159 - val_loss: 0.6636 - val_accuracy: 0.7550\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5234 - accuracy: 0.8148 - val_loss: 0.6627 - val_accuracy: 0.7520\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5188 - accuracy: 0.8205 - val_loss: 0.6688 - val_accuracy: 0.7530\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.5142 - accuracy: 0.8204 - val_loss: 0.6614 - val_accuracy: 0.7530\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5103 - accuracy: 0.8271 - val_loss: 0.6552 - val_accuracy: 0.7590\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5054 - accuracy: 0.8249 - val_loss: 0.6530 - val_accuracy: 0.7550\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5012 - accuracy: 0.8276 - val_loss: 0.6560 - val_accuracy: 0.7610\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4972 - accuracy: 0.8292 - val_loss: 0.6520 - val_accuracy: 0.7540\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4927 - accuracy: 0.8309 - val_loss: 0.6485 - val_accuracy: 0.7530\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4891 - accuracy: 0.8333 - val_loss: 0.6502 - val_accuracy: 0.7620\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4851 - accuracy: 0.8360 - val_loss: 0.6495 - val_accuracy: 0.7650\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4810 - accuracy: 0.8356 - val_loss: 0.6468 - val_accuracy: 0.7610\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4776 - accuracy: 0.8395 - val_loss: 0.6447 - val_accuracy: 0.7580\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.4734 - accuracy: 0.8379 - val_loss: 0.6455 - val_accuracy: 0.7590\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4700 - accuracy: 0.8416 - val_loss: 0.6435 - val_accuracy: 0.7600\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4660 - accuracy: 0.8411 - val_loss: 0.6433 - val_accuracy: 0.7580\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4621 - accuracy: 0.8432 - val_loss: 0.6425 - val_accuracy: 0.7600\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4583 - accuracy: 0.8453 - val_loss: 0.6424 - val_accuracy: 0.7700\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4549 - accuracy: 0.8461 - val_loss: 0.6453 - val_accuracy: 0.7670\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4516 - accuracy: 0.8489 - val_loss: 0.6399 - val_accuracy: 0.7570\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4480 - accuracy: 0.8489 - val_loss: 0.6443 - val_accuracy: 0.7660\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4448 - accuracy: 0.8517 - val_loss: 0.6392 - val_accuracy: 0.7680\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4410 - accuracy: 0.8527 - val_loss: 0.6369 - val_accuracy: 0.7630\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4380 - accuracy: 0.8525 - val_loss: 0.6440 - val_accuracy: 0.7640\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4349 - accuracy: 0.8561 - val_loss: 0.6435 - val_accuracy: 0.7630\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4317 - accuracy: 0.8571 - val_loss: 0.6372 - val_accuracy: 0.7670\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4280 - accuracy: 0.8579 - val_loss: 0.6385 - val_accuracy: 0.7670\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4248 - accuracy: 0.8613 - val_loss: 0.6359 - val_accuracy: 0.7650\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4216 - accuracy: 0.8607 - val_loss: 0.6433 - val_accuracy: 0.7620\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4188 - accuracy: 0.8632 - val_loss: 0.6338 - val_accuracy: 0.7680\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4156 - accuracy: 0.8643 - val_loss: 0.6363 - val_accuracy: 0.7700\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4121 - accuracy: 0.8661 - val_loss: 0.6322 - val_accuracy: 0.7670\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4095 - accuracy: 0.8671 - val_loss: 0.6327 - val_accuracy: 0.7690\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4064 - accuracy: 0.8680 - val_loss: 0.6339 - val_accuracy: 0.7690\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4032 - accuracy: 0.8697 - val_loss: 0.6318 - val_accuracy: 0.7700\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4003 - accuracy: 0.8709 - val_loss: 0.6368 - val_accuracy: 0.7680\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3980 - accuracy: 0.8705 - val_loss: 0.6314 - val_accuracy: 0.7710\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3948 - accuracy: 0.8733 - val_loss: 0.6436 - val_accuracy: 0.7620\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3923 - accuracy: 0.8755 - val_loss: 0.6360 - val_accuracy: 0.7680\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3888 - accuracy: 0.8760 - val_loss: 0.6295 - val_accuracy: 0.7700\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3862 - accuracy: 0.8767 - val_loss: 0.6369 - val_accuracy: 0.7710\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3833 - accuracy: 0.8777 - val_loss: 0.6303 - val_accuracy: 0.7660\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3805 - accuracy: 0.8796 - val_loss: 0.6353 - val_accuracy: 0.7670\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3780 - accuracy: 0.8792 - val_loss: 0.6346 - val_accuracy: 0.7710\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3751 - accuracy: 0.8821 - val_loss: 0.6329 - val_accuracy: 0.7710\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3727 - accuracy: 0.8817 - val_loss: 0.6317 - val_accuracy: 0.7710\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3699 - accuracy: 0.8816 - val_loss: 0.6377 - val_accuracy: 0.7710\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.3675 - accuracy: 0.8831 - val_loss: 0.6329 - val_accuracy: 0.7650\n",
      "Epoch 115/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3644 - accuracy: 0.8840 - val_loss: 0.6321 - val_accuracy: 0.7650\n",
      "Epoch 116/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3624 - accuracy: 0.8855 - val_loss: 0.6336 - val_accuracy: 0.7730\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3597 - accuracy: 0.8869 - val_loss: 0.6340 - val_accuracy: 0.7670\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3572 - accuracy: 0.8872 - val_loss: 0.6389 - val_accuracy: 0.7680\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3551 - accuracy: 0.8884 - val_loss: 0.6375 - val_accuracy: 0.7760\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.3526 - accuracy: 0.8903 - val_loss: 0.6398 - val_accuracy: 0.7740\n"
     ]
    }
   ],
   "source": [
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Store the model's `.history` inside of `model_val_dict`\n",
    "* Check what `keys()` this dictionary contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the final results on the training and testing sets using `model.evaluate()` on `train_final` and `label_train_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 1ms/step - loss: 0.3490 - accuracy: 0.8897\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use this function to get the results on our testing set.  Call the function again, but this time on `test` and `label_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 1ms/step - loss: 0.6335 - accuracy: 0.7800\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the contents of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34896478056907654, 0.8897333145141602]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.33576024494171142, 0.89600000000000002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6335238814353943, 0.7799999713897705]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.72006658554077152, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results. Let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy.\n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g.', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! \n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation accuracy>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA3j0lEQVR4nO3deXhU5fXA8e9J2BeRTUXCLqIoskUU11CpIlYRV3ApiIrgVrD6E7QqVatWqVoq1qLFBVHckSKIgFJcYgUVVBAQECGAsqisAbKc3x/vHXKZzCQTyM3MZM7neeaZueu8N8t77n1XUVWMMcakrrR4J8AYY0x8WSAwxpgUZ4HAGGNSnAUCY4xJcRYIjDEmxVkgMMaYFGeBwMRMRKaLyIDy3jeRichAEfnIt7xdRFrHsu9+fFel+JmZ5FMl3gkwwRKR7b7FWsBuoMBbvk5VJ8Z6LlU9O4h9y0pEGgDPA6cBO4DHVfXhoL7PT1XrlMd5RGQUcISqXuE7d2A/M2NKYoGgkvNnXCKyCrhGVWeF7yciVVQ1vyLTdgBuA2oATYDqQPv4JseUJMn+tlKSFQ2lKBHJEpEcEbldRH4EnhWR+iIyVUQ2isgv3ucM3zFzROQa7/NAEflIREZ7+34vImfv576tRGSuiGwTkVkiMlZEXiwh+fnABlXdqaq/qOrHpVzrUyIyOmzd2yJyi/d5hIis8L5/sYj0LeFcKiJHeJ8bisgUEdkqIp8BbcL2/buIrPG2fy4ip3rrewF3AJd6RU0LI/zM0kTkTyLyg4hsEJEXRKSet62ll44BIrJaRDaJyJ0lpPkcEfnSS8ca72nEv/0UEflERH71tg/01tcUkb95adji/Q5rhv52ws6xSkR6ep9HicjrIvKiiGwFBopINxHJ9r5jvYg8ISLVfMcfIyIzReRnEflJRO4QkcNEZKeINPTt19X7+6wa7XpN2VkgSG2HAQ2AFsBg3N/Ds95ycyAXeKKE408AlgKNgIeBf4uI7Me+LwGfAQ2BUcCVpaT7M6C/iAwqZb+Ql3CZrgCISH3gTGCSt30FcCpQD/gz8KKINInhvGOBXbgnk0Hey28e0An3M34JeE1Eaqjqu8ADwCuqWkdVO0Y490Dv1QNoDdSh+O/iFKAdcAZwt4gcHSWdO4DfAwcD5wBDReR8ABFpDkwH/gE09tK7wDtuNNAVOMm7hv8DCqN8R7g+wOved07EFUcOx/3+u3tpvt5LQ11gFvAucDhwBDBbVX8E5gCX+M57BTBJVfNiTIeJharaK0VewCqgp/c5C9gD1Chh/07AL77lObiiJXCZ1HLftlqAAoeVZV9cwMkHavm2vwi8GCVNRwDrcfUDy4CrvPXVveupF+EYAVYDp3nL1wLvl3DdC4A+vrR/5NumXhrSgTzgKN+2B/z7RjjvL0BH7/Oo8GsM+5nNBq73bWvnfV8VoKWXjgzf9s+AfjH+HTwOPOZ9Hgm8FWGfNNyNQMcI27KAnBL+tkYBc0tJw7DQ9wL9gS+j7Hcp8LH3OR34EehW0f87lf1ldQSpbaOq7gotiEgt4DGgF1DfW11XRNJVtSDC8T+GPqjqTu+GO1plarR9GwE/q+pO375rgGZRznM1MFNV54rIWcCH3rlW4jKTLeEHqKqKyCRchjMXuAwXbELX/XvgFlwG609XSRrjMuU1vnU/+HcQkT8C1+DuchU4KIbzhhwedr4fvO871LfuR9/nnUT52YvICcBDwLFANVzQfM3b3Az3RBSuEa4eJtK2WPh/LojIkcCjQCbuRqAK8HkpaQB4G3hKXEutI4EtqvrZfqbJRGFFQ6ktfOjZP+LuPE9Q1YNwd93g7qiDsh5o4AWhkGhBAFwGkg+gqt/jgtbDwDPAvSUc9zJwkYi0wBVTvQHgLT8N3Ag0VNWDgW8o/Zo3eunwp7V56INXH3A7rlijvnfeLb7zljbs7zpcEZ3/3PnAT6UcF8lLwBSgmarWA57ypWMNYXUbnk24Yq9I23bgMnMARCQdFxj9wq/vn8ASoK33t3VHDGnAu1F5FbgcV2Q4IdJ+5sBYIDB+dXHFAb+Ka6J5T9BfqKo/APOBUSJSTUS6A+eWcMibuPL+870MaCuwEJeRRM1cVfVLXOb9DDBDVX/1NtX2jtsIICJX4e6cS0t3gZeWUSJSS0TaA/4+AHVxGfdGoIqI3I17Igj5CWgpItH+B18GhourSK9DUZ3C/rS+qYt76tolIt1wT0QhE4GeInKJiFTxKsA7qWohMB54VEQOF5F0EekuItVxRXI1vEroqsCfcE8ZpaVhK7BdRI4Chvq2TQUOE5FhIlJdROp6TzEhL+CK6M7D9yRnyo8FAuP3OFATdzf4Ka7yriJcjqtA3AzcD7yC6+9QjKpm4zKye3Bl7jOAacCFwMsi0rmE73kZ6Im7Qw6dbzHwNyAblzl3AEpsheRzI6445kfgOVxFe8gMXCXsMlyxzi72LS4JFc1sFpEvIpx7PO7udy7wvXf8TTGmK9z1wL0isg24G3eHDYCqrgZ6454Gf8bVj4Qqr28FvsZVev8M/BVI84rfrscF1bW4J4R9WhFFcCvu97YN9wT2ii8N24Df4m4AfgS+w1WSh7Z/jKuk/kJVV5Xx2k0MxKuEMSZhiMgrwBJVDfyJxCQHEXkfeElVn4l3WiojeyIwcScix4tIG6/tfC9c08PJcU6WSRAicjzQBd9ThClf1mrIJILDcOXtDXFFDEO9Mn2T4kTkeeB84A9eEZIJgBUNGWNMirOiIWOMSXFJVzTUqFEjbdmyZbyTYYwxSeXzzz/fpKrh/T2AJAwELVu2ZP78+fFOhjHGJBUR+SHaNisaMsaYFGeBwBhjUpwFAmOMSXGB1hF4nYP+jhs+9hlVfShse31cV/o2uC70g1T1m7J+T15eHjk5Oezatav0nU1c1KhRg4yMDKpWtflEjEk0gQUCb0CwsbgxRHKAeSIyxRvbJeQOYIGq9vUGohqLm7CiTHJycqhbty4tW7Yk+rwoJl5Ulc2bN5OTk0OrVq3inRxjTJggi4a64SYjWamqe3CzQfUJ26c9bgIOVHUJbjTGQymjXbt20bBhQwsCCUpEaNiwoT2xGZOgggwETdl3tMUcb53fQuACAG943BZARtg+iMhgEZkvIvM3btwY8cssCCQ2+/0Yk7iCDASR/vPDx7N4CKgvIgtwQ+x+iTfpyD4HqY5T1UxVzWzcOGJ/CGOMqZw2bIAZM+Chh2DWrEC+IsjK4hz2nb0pAzfr0l6quhW4CsCbWPx775VUNm/ezBlnuKqNH3/8kfT0dEIB67PPPqNatWpRj50/fz4vvPACY8aMKfE7TjrpJD755JPyS7QxJr5U4aefYNkyWLrUvX78EXbvhtxcWLsWVq2CX38tOmbECOjZs9yTEmQgmAe0FZFWuMkr+rHvzEiIyMHATq8O4RrchNdbA0xTIBo2bMiCBQsAGDVqFHXq1OHWW2/duz0/P58qVSL/qDMzM8nMzCz1OywIGJPgtm+H//4Xdu6EWrWgZk2oUgXS02H9epfhL1/uPq9fD99/D1t92V2NGtCkiTuuenVo2hROPhnatIFOndyrfv1o335AAgsEqpovIjfiZmpKB8ar6iIRGeJtfwo4GnhBRAqAxbiJySuFgQMH0qBBA7788ku6dOnCpZdeyrBhw8jNzaVmzZo8++yztGvXjjlz5jB69GimTp3KqFGjWL16NStXrmT16tUMGzaMm2++GYA6deqwfft25syZw6hRo2jUqBHffPMNXbt25cUXX0REmDZtGrfccguNGjWiS5curFy5kqlTp+6TrlWrVnHllVeyY8cOAJ544glOOukkAB5++GEmTJhAWloaZ599Ng899BDLly9nyJAhbNy4kfT0dF577TXatIk4vawxlVvoLn3JEvjyS/jGa+leu7bL2D/4wN3Nl6RJE5fBN28Op5wC7drBkUe69+bNIS0+XbsC7UegqtNw0wj61z3l+5wNtC3XLx02DLy783LTqRM8/niZD1u2bBmzZs0iPT2drVu3MnfuXKpUqcKsWbO44447eOONN4ods2TJEj744AO2bdtGu3btGDp0aLG2919++SWLFi3i8MMP5+STT+bjjz8mMzOT6667jrlz59KqVSv69+8fMU2HHHIIM2fOpEaNGnz33Xf079+f+fPnM336dCZPnsz//vc/atWqxc8//wzA5ZdfzogRI+jbty+7du2isLCwzD8HYxJeXh589RVkZ7v3X3+FLVvc+y+/wObN4P1PACACrVu7O/4dO1wwuP56+N3v4JBD3FPBzp1QUAD5+W5d27ZQp068rrBESTfoXDK5+OKLSU9PB2DLli0MGDCA7777DhEhLy8v4jHnnHMO1atXp3r16hxyyCH89NNPZGTs25CqW7due9d16tSJVatWUadOHVq3br23nX7//v0ZN25csfPn5eVx4403smDBAtLT01m2bBkAs2bN4qqrrqJWrVoANGjQgG3btrF27Vr69u0LuE5hxiSl/Hz44QdX5r56tVtXu7bL6N99F2bOdEU7AI0auVe9enDwwS7Db9DA3clnZLjljh2hbt04XUz5q3yBYD/u3INSu3btvZ/vuusuevTowVtvvcWqVavIysqKeEz16tX3fk5PTyc/v1gjqoj7xDrB0GOPPcahhx7KwoULKSws3Ju5q2qxJp42aZFJGoWFrsx97VpX4Roqh1+7FhYvhkWLohfbNGsGl18OPXpA9+5uOcWaO1e+QJCgtmzZQtOmrhvFc889V+7nP+qoo1i5ciWrVq2iZcuWvPJK5Oldt2zZQkZGBmlpaTz//PMUFBQAcOaZZ3Lvvfdy2WWX7S0aatCgARkZGUyePJnzzz+f3bt3U1BQsPepwZgKp+rK6LOzYc0ayMlxGf3Cha6Ixq9qVVcmf9RRcNNN0L49tGpVVBa/YwdUqwZHHJFyGX84CwQV5P/+7/8YMGAAjz76KL/5zW/K/fw1a9bkySefpFevXjRq1Ihu3bpF3O/666/nwgsv5LXXXqNHjx57n1p69erFggULyMzMpFq1avTu3ZsHHniACRMmcN1113H33XdTtWpVXnvtNVq3bl3u6TdmH/n5rrhm4kT47DNXTFOvnsv0168v2u/QQ13Z+9VXu+Ka5s3hsMNcAGjQIOUz+Fgl3ZzFmZmZGj4xzbfffsvRRx8dpxQlju3bt1OnTh1UlRtuuIG2bdsyfPjweCdrL/s9mWKWLYM33nDl9+vXu3b1Gze64p2dO11zyR49XIudX36BFi3gjDPgtNPc3X0JfXTMvkTkc1WN2Fbdnggqkaeffprnn3+ePXv20LlzZ6677rp4J8mkurw8+OQTV5yzfj1s2uTWp6fDvHmuiAegcWN3J3/ooa7d/CGHQFYW9Orl2tSbQFkgqESGDx+eUE8AJoXs2QMff+yGQgg1s9y0CWbP3rfTVP36rrgmP98V4/z1r3DFFXD44fFJtwEsEBhjyiI/H77+Gj791N3R5+S4opwVK2DbNldB26iR27dWLbj0UujdGzIz3d2+zUeRkCwQGGMgNER4qK/I8uXw3HOuSeZBB7my+IULXea/c6fbp3Fj16Y+IwNOOgnOPNOV3ydopykTnQUCY1LNqlVFvWE3bIB//xteesll8K1bu05U8+e7JpYtWriOVjt3wjHHwDXXwAknuPb2LVtaq5xKwgKBMZXdhg1uMLT334f33oOVK/fdHirCad7cdbxatw4eeAB+/3vXm9ZUehYIykFWVhYjR47krLPO2rvu8ccfZ9myZTz55JNRjxk9ejSZmZn07t2bl156iYMPPniffSKNZBpu8uTJHHnkkbRv3x6Au+++m9NOO42eAQxVaxJcQYErylm82A2I9tVXbtytpUvd9jp1XFPM4cNdq5wdO1yZ/bnnujb6JmVZICgH/fv3Z9KkSfsEgkmTJvHII4/EdPy0adNK3ymKyZMn87vf/W5vILj33nv3+1wmSbzyCvzjH65HbPfuro39e+8VDYEc0qoVdOgAV10Fp58OXbtaZa2JKD5jniaC7Gx48MGidswH4KKLLmLq1Kns9sYyWbVqFevWreOUU05h6NChZGZmcswxx3DPPfdEPL5ly5Zs8tpX/+Uvf6Fdu3b07NmTpaE7OVwfgeOPP56OHTty4YUXsnPnTj755BOmTJnCbbfdRqdOnVixYgUDBw7k9ddfB2D27Nl07tyZDh06MGjQoL3pa9myJffccw9dunShQ4cOLFmypFiaVq1axamnnkqXLl3o0qXLPvMhPPzww3To0IGOHTsyYsQIAJYvX07Pnj3p2LEjXbp0YcWKFQf8czW4IRVCY+f88AP07w/9+rninmnTYMgQd4e/YoXL8MePdy16tmxxRUBvvw233w4nnmhBwESnqkn16tq1q4ZbvHhxsXUl+uQT1Zo1VdPT3fsnn5Tt+Ah69+6tkydPVlXVBx98UG+99VZVVd28ebOqqubn5+vpp5+uCxcuVFXV008/XefNm6eqqi1atNCNGzfq/Pnz9dhjj9UdO3boli1btE2bNvrII4+oquqmTZv2ftedd96pY8aMUVXVAQMG6GuvvbZ3W2g5NzdXMzIydOnSpaqqeuWVV+pjjz229/tCx48dO1avvvrqYtezY8cOzc3NVVXVZcuWaejnPm3aNO3evbvu2LFjn+vr1q2bvvnmm6qqmpubu3e7X5l/T6num29Ue/RQdeHAvapUUb3/ftW8PNXCQtUVK1RXrYp3Sk0SAOZrlHw1NYuG5sxxHWAKCtz7nDnuEfsAhIqH+vTpw6RJkxg/fjwAr776KuPGjSM/P5/169ezePFijjvuuIjn+PDDD+nbt+/eQd3OO++8vdu++eYb/vSnP/Hrr7+yffv2fYqhIlm6dCmtWrXiyCOPBGDAgAGMHTuWYcOGAXDBBRcA0LVrV958881ix9tw1QEqLIT//Ac+/9w1zaxZE777Dr74wg2xcPjhrsx+6lS3/f77Xdv8/Hw3tEKHDkXnsnGfTDlIzUCQleXaRe/Z496jDAldFueffz633HILX3zxBbm5uXTp0oXvv/+e0aNHM2/ePOrXr8/AgQPZFWqvHUX4UNAhAwcOZPLkyXTs2JHnnnuOOXPmlHgeLWUMqdBQ1tGGurbhqg/Qnj2uiCYjwzWzBDck8scfuyLJr77ad/9atdygaccd54qBvv7aNdUMBQFjAhRoHYGI9BKRpSKyXERGRNheT0T+IyILRWSRiFwVZHr26t7ddX2/7z73foBPA+CmkszKymLQoEF7ZwfbunUrtWvXpl69evz0009Mnz69xHOcdtppvPXWW+Tm5rJt2zb+85//7N22bds2mjRpQl5eHhMnTty7vm7dumzbtq3YuY466ihWrVrF8uXLAZgwYQKnn356zNezZcsWmjRpQlpaGhMmTNhnuOrx48ez06uU/PnnnznooIP2DlcNsHv37r3bU9KHH0Lnzq6Ctk0b11qnbl3XPLN/fzcu/osvumCxdatrrrl1qxuT57XX4KOPXJn/U09ZEDAVIrAnAhFJB8YCvwVygHkiMkVVF/t2uwFYrKrnikhjYKmITFQ3mX2wuncvlwDg179/fy644AImTZoEQMeOHencuTPHHHMMrVu35uSTTy7x+NDcxp06daJFixaceuqpe7fdd999nHDCCbRo0YIOHTrszfz79evHtddey5gxY/ZWEoMrnnn22We5+OKLyc/P5/jjj2fIkCExX4sNVx2j/HyXcX/4ocvQV650LXhatIAXXnCZ/eLFrjioXTs4+mg3V603cx1Vq1aqma5McgpsGGoR6Q6MUtWzvOWRAKr6oG+fkUAzXEBoCcwEjlTVqBPj2jDUyavS/J7WrXOds2bNcuX4mze79Y0auRE0zz0X7rzT9dw1JkHEaxjqpsAa33IOcELYPk8AU4B1QF3g0khBQEQGA4MBmjdvHkhijSlm8WJ45x1XfPjRR0Vt9EM3T/Xrw9lnQ9++brhkG2PHJKkgA0GkWs/wx4+zgAXAb4A2wEwR+VBVt+5zkOo4YBy4J4LyT6pJadu2ufL5NWtc+f2GDfDWW67CFtwUhwMGQMOGbvngg10Dg06d3Hg8xiS5IANBDq7YJyQDd+fvdxXwkNfGdbmIfA8cBXxW1i+L1JrFJI6Ealn09dfuDv+rr1wTzi++cE2J/U4+2fXeveACGyvfVHpBBoJ5QFsRaQWsBfoBl4Xtsxo4A/hQRA4F2gFhI2KVrkaNGmzevJmGDRtaMEhAqsrmzZvj17+goMD1tP30Uxg9Gj74wK2vV8812Rwxwt3ht2vnZsOqXdvK901KCSwQqGq+iNwIzADSgfGqukhEhnjbnwLuA54Tka9xRUm3q+qmsn5XRkYGOTk5bNy4sRyvwJSnGjVqkJGREfwXrVnjyvT/9z9Xxv/tt26mrNATSdOm8MgjcMkl0KyZDaNsElt2tuvwmpVV7q0c/SrF5PUmhW3ZAjNnusx/9mzXQxfc3f4xx7jmmk2bQoMGrkln79424blJLNEy++xsN9FPqOPr44+7Fmr7GRRs8npTOfz4I6xe7YZd2LkTnnnGtdXfscO12Dn9dBg61P3zHHusVeQGLda71fK+q/WfD4qfO9L2hg2LMtFI60o6Ntr3lJSeSN8X7Vh/Zh/q4JqdDaNGucYLhYXu/cYb3Wf/fuXEAoFJfLt2ueKcBx4omlIRXHl+//5w9dVu1iwbXTM25ZEx+zOw9HQYNMhNZBOeoTZsCMOGFc/oSksXRM6sw79XxHXqC90xf/klPPusWxfanpfnMtC0NKhSpfi66tVduiDyucM/h64V3I1ILN8X6dg5c9yNTWjcs927XeZ/4YXuZxYKAmlp7lVQ4JbLaXw0PysaMomroMANuXDXXW4O3YsvhiuvdFMn5uW5NvyNG8c7lYmjpAw+UsYcKQOP1YMPut9LqLWViJvvODxDFXGZVyhD69nTZXawb6ZfWoZavbrL6N94w3XkKywsqt9Rdfukp7vjQ3maf3tIpHXp6XDtta5XeKRzhx8j4m46RNw1luX7/MdGu9a0tOI/s1BwiCWgRlFS0ZAFApM49uxxE6SvWePG2nn6aVfm3749PPaYmxy9sjiQu/JIx0YrYgjf5s+YoSgDD5U/RyvSCH0ObQ8FlF27ijK5SBlqKIMO3c1GukuOJUMt7TyRriuU4Zb2RFDSuljTWNr3RTu2pJ9ZeDHQAT7JWSAwie3bb115/4QJ4G/51bmzG6qhb9/4l/f776hLKleOpazZf/db2j96+HnCjw1l4KtXu8BZUFD8znvUqOKZjP/OOdbMOtIdemlFMaXdyceaoUa6S/Y/WUR70vFvj1ZHUNLPrrSnlmhFPuHfF+3Y0O8fyrViOBILBCbxFBbCjBnuTn/mTJfxnHeem0S9bVs3fHOjRvFp3hme6YcyGX+ZbaSMsKRy6mh3lqE7QojtPP5j/XeOZSkPDz93+N00lJxZh9J9330wcmTRzytahlpS2X6sGWosdQ37e8dc0tNUtL+N0iqB9+fYgJuKWiAw8bdnj2vP/8MPbviGV15xGUeTJq41xDXXuAnV4y2UKYRX1IVnlCXdWUcrp461rDmW8m6RoiAQCgQllXOHZ8yha/XfTfuvOZYngvAMs7QMdX9a5IT/boLKKCuovX48WSAw8fHrrzBxortLXLiwaH2VKq68/4orXCVYvNr1R8qY/He1IZGKTsKDQ3jGHFoXqWLQf/cL+35frOcZNMgVnYXfJUP0VjWx3OnGWkcQLcNMgQw1WVkgMBVH1Q3lMG6cu+vPzYUuXdzQzIcdBoce6qZbDA3gFrRoGVxJ5b3RilP8lamRyqT9GXOkYo5Id7+RmmHGcp79qZ+wjDmlWSAwwfroI/j7393d9A8/uHl369SByy6DwYOha9fy+Z7S7jbDt0cq5om1FUfz5vt397s/d8TldR5jSmCBwARjxw5XWfjEE658v2NHV8l74onQr1/5zLwVa8ekSHfWULyYp7R23QH02jQmEdgQE6Z87dgB//636+2bkwM33eR6/ZZlYpayNpX0t2zx96z0t1gJ9dAsKIB//ctl8FWquEw/1p6edgduUpAFAhObtWvd8M1z5sDkya7I5NRT4aWX3HussrMjt6OH4pWckVrSiLhjGjZ04wr5y/mrVHEZvqp7FRRELuaByJm+BQCToiwQmOhUXSY9Zoybm1fVzc7Vs6crpjn55NjOE1684++NumePCwwrVxaV5Yday/iLb8IrbMPPA5Hb40cbPsEyfWP2skBgItu40VX2zprlxvO54w646CLo0MHdfUcTrTOWf3iD8Lbwzz4b20Bdocz7wQeLV/b6M/3f/96KeYwpAwsEprjPPnOZ/oYNbrrGa65x49FEU1LHpPChAULFO+EVuiUNShaemWdluYw/2sBp3btbADCmDCwQGGfFCnj1VZg2zU3k3qwZfPxxUdPPaJW70Sp0oejdX7YfPoZKdjY8/3xRayB/z9domXn37q7Iyu76jSkXgTYfFZFewN9xU1U+o6oPhW2/DbjcW6wCHA00VtWfo53Tmo+Ws5Ur4f77XYZeUOA6M51zjruzD3X6ijR0ALh1/nL6SD1w/WX71hvVmLiJS/NREUkHxgK/BXKAeSIyRVUXh/ZR1UeAR7z9zwWGlxQETDn75z/h5ptdmfxNN8Ef/+j6AYSbM6f45BmtWxcvp4/UAzeWzN2KcoyJqyCLhroBy1V1JYCITAL6AIuj7N8feDnA9JiQggK49VaXaZ9zjhsO4vDDo+8fKpMPlf3PmuWCRxXvzydSOb0xJmkEGQiaAmt8yznACZF2FJFaQC/gxijbBwODAZo3b16+qUw1ubluese333bFP6NHR28F5C+ymT1733Ht/W30rUjHmKQWZCCINJB8tAqJc4GPoxULqeo4YBy4OoLySV4K2rLFjfn/4YeuNdCNEeOuE6leYNQod2xonT0BGFMpBBkIcoBmvuUMYF2UffthxULBWrPGBYFFi+Dll90EMNFkZ7tMP1QUFBrSYeRIa61jTCUUZCCYB7QVkVbAWlxmf1n4TiJSDzgduCLAtKSuwkI37s7tt7vP//kPnHVW9P0jjdhZrVrR0AxWsWtMpRNYIFDVfBG5EZiBaz46XlUXicgQb/tT3q59gfdUdUdQaUlZ27fD+ee7u/iePV2lcKtWkfcNH7wtvIOXZf7GVFo2DHVltXu3mwxm9mzXTPTaa4vP/xtpiOeyzGpljEkaNgx1qsnPd+MEzZzpev0OHLjv9tJ6BFtrIGNSigWCykbVtQZ680147LHIQSBaj+DwwduMMSnBAkFl8+ijRZXDw4YVrQ+vA4jWI9ieAoxJORYIKpPJk+G229zIoQ88ULQ+fBpH6xFsjPGxQFBZrFwJl18Oxx/vyv/T0oq2+ccKAqsDMMbswwJBZRCqF0hLg9dfh5o1i7ZlZ7vioNBTgNUBGGPCWCCoDN58E6ZPd/UDzXyducOLhK691oKAMaYYCwTJbts2+MMfoGNHN5S0X3iRUPPmFgSMMcVYIEh2d90F69bBG28UFf+E+Kd09A8TYYwxPhYIktmnn8KYMTB0KJzgG+E7fPhoGyTOGFMCCwTJavdu1/QzIwMe8mYADe8xHBoiYuTI+KbVGJPQLBAkq7/8Bb791k02X7du5B7DoeGj7UnAGFOCtNJ3MQln2TJ48EG48ko4+2y3LlQx7O8xbPUCxpgYWCBIRvff7zL50aOL1oUqhtPT3ft119nIocaYmFjRULL57juYOBGGD4dDDila3727VQwbY/aLBYJk88ADbpC4225zy/4WQjZ7mDFmP1ggSCYrVsCECXDzzW5soVGjircQskBgjCkjCwTJ5KGHoGpV6NHDWggZY8pNoJXFItJLRJaKyHIRGRFlnywRWSAii0Tkv0GmJ6n99JPrIzBoEHzzjbUQMsaUm8CeCEQkHRgL/BbIAeaJyBRVXezb52DgSaCXqq4WkUMinszAk09CXp6bbGbTpqKhI2xOAWPMAQqyaKgbsFxVVwKIyCSgD7DYt89lwJuquhpAVTcEmJ7klZvrAsG550Lbtu5lLYSMMeUkyEDQFFjjW84BTgjb50igqojMAeoCf1fVF8JPJCKDgcEAzZs3DySxCW3iRPcUMHx40TprIWSMKSdB1hFIhHUatlwF6AqcA5wF3CUiRxY7SHWcqmaqambjxo3LP6WJTNXNM9C5sysOevBB12TUGGPKSZBPBDmAb5YUMoB1EfbZpKo7gB0iMhfoCCwLMF3JZcYMN6bQXXdBz55FQ0pbU1FjTDkJ8olgHtBWRFqJSDWgHzAlbJ+3gVNFpIqI1MIVHX0bYJqSz+OPQ5MmRZXDBQVFTUWNMaYcBBYIVDUfuBGYgcvcX1XVRSIyRESGePt8C7wLfAV8Bjyjqt8Elaaks3ixeyK44QbXb8A/lpA1FTXGlBNRDS+2T2yZmZk6f/78eCejYgwe7HoSr1kDjRoVH07CGGNiJCKfq2pmpG3WszhRbdrkgsBZZ8HTT9tYQsaYwFggSFT/+pcbQmLGDJg61SqIjTGBsfkIEtHu3TB2rOs4lpdnFcTGmEBZIEhEL78M69e7SmKrIDbGBMyKhhKNqpt5rE0b2LHDNR/dvNkqiI0xgbFAkGhmzIBFi9wTwN13W92AMSZwVjSUaP72N6hb19ULWN2AMaYCWCBIJM8/D7NmuaEkrG7AGFNBYioaEpHaQK6qFnqDwh0FTFfVvEBTl0qys+Hqq93nd9+1ugFjTIWJtY5gLm5MoPrAbGA+cClweVAJSznTp7uiIHDFQZs3w8iR8U2TMSYlxFo0JKq6E7gA+Ieq9gXaB5esFLR1q3u34iBjTAWL9YlARKQ77gng6jIea0qj6loLtW8PV1xhxUHGmAoVa2Y+DBgJvOWNINoa+CCwVKWaOXNgyRJ47jkYMCDeqTHGpJiYAoGq/hf4L4CIpOEmk7k5yISllCefhPr14ZJL4p0SY0wKiqmOQEReEpGDvNZDi4GlInJbsElLERs3wuTJMHAg1KwZ79QYY1JQrJXF7VV1K3A+MA1oDlwZVKJSyquvQn6+azFkcxEbY+Ig1kBQVUSq4gLB217/geSa0SZRPfkkiLjRRs84w4KBMabCxRoI/gWsAmoDc0WkBbC1tINEpJeILBWR5SIyIsL2LBHZIiILvNfdZUl80lu2zE1HCTachDEmbmKtLB4DjPGt+kFEepR0jIikA2OB3wI5wDwRmaKqi8N2/VBVf1eGNFceEye69+rV3bwD1n/AGBMHsVYW1xORR0Vkvvf6G+7poCTdgOWqulJV9wCTgD4HmN7KQxVefNGNK/T++3DffTbKqDEmLmItGhoPbAMu8V5bgWdLOaYpsMa3nOOtC9ddRBaKyHQROSbSiURkcCgIbdy4McYkJ7inn4aVK+HEE13mP3KkBQFjTFzEGgjaqOo93t39SlX9M9C6lGMkwrrwCuYvgBaq2hH4BzA50olUdZyqZqpqZuPGjWNMcgLLznazj4EbdtoqiI0xcRRrIMgVkVNCCyJyMpBbyjE5QDPfcgawzr+Dqm5V1e3e52m41kmNYkxT8vrgA9dkFKyC2BgTd7EOMTEEeEFE6nnLvwCljYUwD2grIq2AtUA/4DL/DiJyGPCTqqqIdMMFps2xJj5ptWnj3tPSrILYGBN3sbYaWgh0FJGDvOWtIjIM+KqEY/JF5EZgBpAOjPfGKRribX8KuAgYKiL5uCeMfqpa+fsnrF/v3m+5BS64wOoGjDFxJfub74rIalVtXs7pKVVmZqbOnz+/or+2fJ15JuTkFPUhMMaYgInI56qaGWnbgUxVGaky2JRm+3b473/hnHPinRJjjAEOLBBU/iKcILz/vqsg7t073ikxxhiglDoCEdlG5AxfABsqc3+88w7UrQsnnxzvlBhjDFBKIFDVuhWVkJSgCtOmwW9/61oLGWNMAjiQoiFTVl9/7SqJwTqRGWMShgWCivTEE+797bdtyGljTMKwQFCRpk1z7zbktDEmgcTas9gcqPXrYe1aqFoVCgutR7ExJmFYIKgoU6e69/HjYc0aFwSsR7ExJgFYIKgI2dnw6KNw2GFw+eVuakpjjEkQVkcQtOxsVzG8ZAls2gSffhrvFBljzD4sEARtzhzYvdt9Liy0CmJjTMKxQBC0rKyioqDq1a2C2BiTcCwQBK1rV6hVCzp2tDmJjTEJyQJB0GbNgm3b4P77LQgYYxKSBYKgTZoEBx/s5iAwxpgEZIEgSLt2weTJbhYyG2TOGJOgAg0EItJLRJaKyHIRGVHCfseLSIGIXBRkeirc44+7YqEOHeKdEmOMiSqwQCAi6cBY4GygPdBfRNpH2e+vuLmNK4/sbPjTn9znO+6wAeaMMQkryCeCbsByVV2pqnuASUCfCPvdBLwBbAgwLRXvvffc4HJgA8wZYxJakIGgKbDGt5zjrdtLRJoCfYGnAkxHfBQWuve0NBtgzhiT0IIcayjSgDrh014+DtyuqgVSwvg7IjIYGAzQvHnz8kpfsD76CA4/HG64AXr0sKajxpiEFWQgyAGa+ZYzgHVh+2QCk7wg0AjoLSL5qjrZv5OqjgPGAWRmZkaaQzlxZGfDW2+5ServvdfVDxhjTAILMhDMA9qKSCtgLdAPuMy/g6q2Cn0WkeeAqeFBIKmEBpjbtcstH3tsfNNjjDExCCwQqGq+iNyIaw2UDoxX1UUiMsTbXvnqBebMcRXD6j20LFkS1+QYY0wsAp2PQFWnAdPC1kUMAKo6MMi0VIisLKhSxbUWsgpiY0ySsJ7F5al7dzjtNKhRA2bMsApiY0xSsEBQnn74AT74AIYMsacBY0zSsEBQnkaPdnMP3HJLvFNijDExs0BQXjZsgGeegSuugGbNSt/fGGMShAWC8pCdDZde6pqN3n57vFNjjDFlEmiroZQQ6juQmwvp6fDzz/FOkTHGlIk9ERyoOXOKOpCFlo0xJolYIDhQXbsWdSCzvgPGmCRkgeBAffyxe7/pJpuc3hiTlKyO4EC8+y489JAbXXTMmHinxhhj9os9Eeyv7Gw491w3tlB2ts1AZoxJWvZEUFbZ2a5CeMkSyM936/Ly3DorFjLGJCELBGURaiq6Z0/RuvR0qyQ2xiQ1CwRlERpmOjQXcdu2cNVVLgjY04AxJklZICiLrCx3979rl2syev/9cMkl8U6VMcYcEKssLovu3eGVV1xx0NlnWxAwxlQKFgjKau5cKCy05qLGmErDioZilZ3t+g08+SRcfDEccUS8U2SMMeUi0EAgIr2Av+PmLH5GVR8K294HuA8oBPKBYar6UZBp2i/+SelV4cwz450iY4wpN4EFAhFJB8YCvwVygHkiMkVVF/t2mw1MUVUVkeOAV4GjgkrTfguflP6nn+KaHGOMKU9B1hF0A5ar6kpV3QNMAvr4d1DV7aqh3JXagJKIsrJcBTFYnwFjTKUTZCBoCqzxLed46/YhIn1FZAnwDjAo0olEZLCIzBeR+Rs3bgwksSU68URo1QoaNYL337c+A8aYSiXIQCAR1hW741fVt1T1KOB8XH1B8YNUx6lqpqpmNm7cuHxTGYsFC2DpUvjzn+Hkkyv++40xJkBBVhbnAP7JezOAddF2VtW5ItJGRBqp6qYA0xW70LhCCxa4IqF+/eKdImOMKXdBBoJ5QFsRaQWsBfoBl/l3EJEjgBVeZXEXoBqwOcA0xc4/rlBBgasXaNAg3qkyxphyF1ggUNV8EbkRmIFrPjpeVReJyBBv+1PAhcDvRSQPyAUu9VUex1f4uELNmpW4uzHGJKtA+xGo6jRgWti6p3yf/wr8Ncg07LfwcYWuuSbeKTLGmEDYEBORhOoGHngA0tLgoovgtNPinSpjjAmEDTERzl83kJbmioZuvz3eqTLGmMBYIAjnrxsoKIDDD4euXeOdKmOMCYwVDYUL1Q2I1w1ixIiiz8YYUwlZIAjXvTu88YYLBiedBDfdFO8UGWNMoKxoKCRUQZyVBdOnu4npn3km3qkyxpjAWSCAfSuIq1aFvDzXXPToo+OdMmOMCZwFAti3griw0K274464JskYYyqK1RFAUQVxerrrPJaVBc2bxztVxhhTISwQgKsgnj0bzj3XLd97b3zTY4wxFcgCQciJJ8KyZdCliw01bYxJKVZHEDJrFixeDM8/b/0GjDEpxZ4IQp54Ag45BC69NN4pMcaYCmWBAGDdOnjnHRg0CKpXj3dqjDGmQlkgyM6Gq65yTUcHRZwy2RhjKrXUriMIdSTLzXUjjW7aBG3bxjtVxhhToVL7iWDOHNi9e99lY4xJMYEGAhHpJSJLRWS5iIyIsP1yEfnKe30iIh2DTE8xWVlFLYSqV3fLxhiTYgILBCKSDowFzgbaA/1FpH3Ybt8Dp6vqccB9wLig0hNRu3auSOjEE12Hsu7dK/TrjTEmEQRZR9ANWK6qKwFEZBLQB1gc2kFVP/Ht/ymQEWB6ioRGGt20yQ0w989/QqdOFfLVxhiTaIIMBE2BNb7lHOCEEva/GpgeaYOIDAYGAzQ/0DGA/CONFhbCEUdYEDDGpLQg6wgidc/ViDuK9MAFgoiTA6vqOFXNVNXMxo0bH1iq/CONqrriIWOMSWFBBoIcoJlvOQNYF76TiBwHPAP0UdXNAabHCZ+KcujQwL/SGGMSWZCBYB7QVkRaiUg1oB8wxb+DiDQH3gSuVNVlAaalSPfuMGMG1KkDp5wC55xTIV9rjDGJKrA6AlXNF5EbgRlAOjBeVReJyBBv+1PA3UBD4Elxd+j5qpoZVJr22rEDtm2D4cMD/ypjjEl0ohqx2D5hZWZm6vz58w/sJP37w8yZboyhatXKJ2HGGJPAROTzaDfaqdezuKDAFQ316WNBwBhjSMVAsHAh/PKLa0JqjDEmhQadC3UiW7vWLffoEdfkGGNMokiNQODvRKYKLVpAkybxTpUxxiSE1AgE/k5kAIcdFtfkGGNMIkmNOoJQJ7I073LPOy+uyTHGmESSGoGge3c3umiogvi66+KbHmOMSSCpEQjABYOCAujYERo2jHdqjDEmYaROINi1Cz7+GH7zm3inxBhjEkrqBILsbDctpQUCY4zZR+oEgqpVoXdvOPXUeKfEGGMSSmo0HwU30ug778Q7FcYYk3BS54nAGGNMRBYIjDEmxVkgMMaYFGeBwBhjUpwFAmOMSXEWCIwxJsVZIDDGmBRngcAYY1Jc0k1eLyIbgR/KeFgjYFMAyYkHu5bEZNeSuCrT9RzItbRQ1caRNiRdINgfIjJfVTPjnY7yYNeSmOxaEldlup6grsWKhowxJsVZIDDGmBSXKoFgXLwTUI7sWhKTXUviqkzXE8i1pEQdgTHGmOhS5YnAGGNMFBYIjDEmxVXqQCAivURkqYgsF5ER8U5PWYhIMxH5QES+FZFFIvIHb30DEZkpIt957/XjndZYiUi6iHwpIlO95WS+loNF5HURWeL9jron6/WIyHDvb+wbEXlZRGoky7WIyHgR2SAi3/jWRU27iIz08oOlInJWfFIdWZRrecT7G/tKRN4SkYN928rtWiptIBCRdGAscDbQHugvIu3jm6oyyQf+qKpHAycCN3jpHwHMVtW2wGxvOVn8AfjWt5zM1/J34F1VPQroiLuupLseEWkK3AxkquqxQDrQj+S5lueAXmHrIqbd+//pBxzjHfOkl08kiucofi0zgWNV9ThgGTASyv9aKm0gALoBy1V1paruASYBfeKcppip6npV/cL7vA2X0TTFXcPz3m7PA+fHJYFlJCIZwDnAM77VyXotBwGnAf8GUNU9qvorSXo9uClra4pIFaAWsI4kuRZVnQv8HLY6Wtr7AJNUdbeqfg8sx+UTCSHStajqe6qa7y1+CmR4n8v1WipzIGgKrPEt53jrko6ItAQ6A/8DDlXV9eCCBXBIHJNWFo8D/wcU+tYl67W0BjYCz3pFXc+ISG2S8HpUdS0wGlgNrAe2qOp7JOG1+ERLe7LnCYOA6d7ncr2WyhwIJMK6pGsrKyJ1gDeAYaq6Nd7p2R8i8jtgg6p+Hu+0lJMqQBfgn6raGdhB4hadlMgrP+8DtAIOB2qLyBXxTVVgkjZPEJE7ccXFE0OrIuy239dSmQNBDtDMt5yBe+RNGiJSFRcEJqrqm97qn0Skibe9CbAhXukrg5OB80RkFa6I7jci8iLJeS3g/rZyVPV/3vLruMCQjNfTE/heVTeqah7wJnASyXktIdHSnpR5gogMAH4HXK5FHb/K9VoqcyCYB7QVkVYiUg1XsTIlzmmKmYgIrgz6W1V91LdpCjDA+zwAeLui01ZWqjpSVTNUtSXu9/C+ql5BEl4LgKr+CKwRkXbeqjOAxSTn9awGThSRWt7f3Bm4+qhkvJaQaGmfAvQTkeoi0gpoC3wWh/TFTER6AbcD56nqTt+m8r0WVa20L6A3rqZ9BXBnvNNTxrSfgnvU+wpY4L16Aw1xLSG+894bxDutZbyuLGCq9zlprwXoBMz3fj+TgfrJej3An4ElwDfABKB6slwL8DKubiMPd5d8dUlpB+708oOlwNnxTn8M17IcVxcQygOeCuJabIgJY4xJcZW5aMgYY0wMLBAYY0yKs0BgjDEpzgKBMcakOAsExhiT4iwQGOMRkQIRWeB7lVtvYRFp6R9V0phEUiXeCTAmgeSqaqd4J8KYimZPBMaUQkRWichfReQz73WEt76FiMz2xoqfLSLNvfWHemPHL/ReJ3mnSheRp72x/98TkZre/jeLyGLvPJPidJkmhVkgMKZIzbCioUt927aqajfgCdxIqnifX1A3VvxEYIy3fgzwX1XtiBuDaJG3vi0wVlWPAX4FLvTWjwA6e+cZEsylGROd9Sw2xiMi21W1ToT1q4DfqOpKbyDAH1W1oYhsApqoap63fr2qNhKRjUCGqu72naMlMFPdZCmIyO1AVVW9X0TeBbbjhqqYrKrbA75UY/ZhTwTGxEajfI62TyS7fZ8LKKqjOwc3m15X4HNvghhjKowFAmNic6nvPdv7/AluNFWAy4GPvM+zgaGwd57mg6KdVETSgGaq+gFu4p6DgWJPJcYEye48jClSU0QW+JbfVdVQE9LqIvI/3M1Tf2/dzcB4EbkNN2PZVd76PwDjRORq3J3/UNyokpGkAy+KSD3cZCOPqZv20pgKY3UExpTCqyPIVNVN8U6LMUGwoiFjjElx9kRgjDEpzp4IjDEmxVkgMMaYFGeBwBhjUpwFAmOMSXEWCIwxJsX9P6MV1NrXS2LVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['accuracy'] \n",
    "val_acc_values = model_val_dict['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r.', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Early stopping\n",
    "\n",
    "Now that we know that the model starts to overfit around epoch 60, we can just retrain the model from scratch, but this time only up to 60 epochs! This will help us with our overfitting problem.  This method is called **_Early Stopping_**.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the exact model we did above. \n",
    "* Compile the model with the exact same hyperparameters.\n",
    "* Fit the model with the exact same hyperparameters, with the exception of `epochs`.  This time, set epochs to `60` instead of `120`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "30/30 [==============================] - 1s 16ms/step - loss: 1.9420 - accuracy: 0.1541 - val_loss: 1.9346 - val_accuracy: 0.1810\n",
      "Epoch 2/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.9239 - accuracy: 0.1800 - val_loss: 1.9204 - val_accuracy: 0.1890\n",
      "Epoch 3/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.9069 - accuracy: 0.2128 - val_loss: 1.9048 - val_accuracy: 0.2290\n",
      "Epoch 4/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8884 - accuracy: 0.2396 - val_loss: 1.8865 - val_accuracy: 0.2560\n",
      "Epoch 5/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.8676 - accuracy: 0.2569 - val_loss: 1.8664 - val_accuracy: 0.2740\n",
      "Epoch 6/60\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.8445 - accuracy: 0.2728 - val_loss: 1.8428 - val_accuracy: 0.2910\n",
      "Epoch 7/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8176 - accuracy: 0.2977 - val_loss: 1.8159 - val_accuracy: 0.3090\n",
      "Epoch 8/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.7861 - accuracy: 0.3263 - val_loss: 1.7837 - val_accuracy: 0.3200\n",
      "Epoch 9/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.7503 - accuracy: 0.3517 - val_loss: 1.7475 - val_accuracy: 0.3510\n",
      "Epoch 10/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.7106 - accuracy: 0.3803 - val_loss: 1.7082 - val_accuracy: 0.3730\n",
      "Epoch 11/60\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.6676 - accuracy: 0.4047 - val_loss: 1.6657 - val_accuracy: 0.3950\n",
      "Epoch 12/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.6217 - accuracy: 0.4273 - val_loss: 1.6205 - val_accuracy: 0.4160\n",
      "Epoch 13/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5740 - accuracy: 0.4463 - val_loss: 1.5740 - val_accuracy: 0.4470\n",
      "Epoch 14/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5254 - accuracy: 0.4779 - val_loss: 1.5253 - val_accuracy: 0.4610\n",
      "Epoch 15/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.4765 - accuracy: 0.4975 - val_loss: 1.4781 - val_accuracy: 0.4940\n",
      "Epoch 16/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4274 - accuracy: 0.5260 - val_loss: 1.4321 - val_accuracy: 0.5200\n",
      "Epoch 17/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3799 - accuracy: 0.5515 - val_loss: 1.3849 - val_accuracy: 0.5480\n",
      "Epoch 18/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3331 - accuracy: 0.5824 - val_loss: 1.3414 - val_accuracy: 0.5500\n",
      "Epoch 19/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.2883 - accuracy: 0.6017 - val_loss: 1.2973 - val_accuracy: 0.5830\n",
      "Epoch 20/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.2443 - accuracy: 0.6204 - val_loss: 1.2558 - val_accuracy: 0.5990\n",
      "Epoch 21/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.2027 - accuracy: 0.6395 - val_loss: 1.2182 - val_accuracy: 0.6100\n",
      "Epoch 22/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1625 - accuracy: 0.6539 - val_loss: 1.1769 - val_accuracy: 0.6480\n",
      "Epoch 23/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1242 - accuracy: 0.6689 - val_loss: 1.1426 - val_accuracy: 0.6520\n",
      "Epoch 24/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0871 - accuracy: 0.6773 - val_loss: 1.1065 - val_accuracy: 0.6650\n",
      "Epoch 25/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0528 - accuracy: 0.6889 - val_loss: 1.0752 - val_accuracy: 0.6640\n",
      "Epoch 26/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0196 - accuracy: 0.6983 - val_loss: 1.0460 - val_accuracy: 0.6770\n",
      "Epoch 27/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9889 - accuracy: 0.7053 - val_loss: 1.0169 - val_accuracy: 0.6910\n",
      "Epoch 28/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9597 - accuracy: 0.7107 - val_loss: 0.9931 - val_accuracy: 0.6830\n",
      "Epoch 29/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9323 - accuracy: 0.7195 - val_loss: 0.9676 - val_accuracy: 0.6920\n",
      "Epoch 30/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9062 - accuracy: 0.7247 - val_loss: 0.9473 - val_accuracy: 0.6960\n",
      "Epoch 31/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8821 - accuracy: 0.7312 - val_loss: 0.9248 - val_accuracy: 0.7050\n",
      "Epoch 32/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8597 - accuracy: 0.7333 - val_loss: 0.9076 - val_accuracy: 0.7050\n",
      "Epoch 33/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8385 - accuracy: 0.7411 - val_loss: 0.8849 - val_accuracy: 0.7090\n",
      "Epoch 34/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8186 - accuracy: 0.7487 - val_loss: 0.8700 - val_accuracy: 0.7090\n",
      "Epoch 35/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8001 - accuracy: 0.7520 - val_loss: 0.8556 - val_accuracy: 0.7140\n",
      "Epoch 36/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7826 - accuracy: 0.7560 - val_loss: 0.8401 - val_accuracy: 0.7210\n",
      "Epoch 37/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7662 - accuracy: 0.7601 - val_loss: 0.8302 - val_accuracy: 0.7220\n",
      "Epoch 38/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7509 - accuracy: 0.7633 - val_loss: 0.8183 - val_accuracy: 0.7280\n",
      "Epoch 39/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7366 - accuracy: 0.7653 - val_loss: 0.8039 - val_accuracy: 0.7230\n",
      "Epoch 40/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7226 - accuracy: 0.7715 - val_loss: 0.7919 - val_accuracy: 0.7330\n",
      "Epoch 41/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7098 - accuracy: 0.7739 - val_loss: 0.7843 - val_accuracy: 0.7290\n",
      "Epoch 42/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6977 - accuracy: 0.7735 - val_loss: 0.7769 - val_accuracy: 0.7340\n",
      "Epoch 43/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6863 - accuracy: 0.7784 - val_loss: 0.7687 - val_accuracy: 0.7330\n",
      "Epoch 44/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6757 - accuracy: 0.7819 - val_loss: 0.7608 - val_accuracy: 0.7360\n",
      "Epoch 45/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6653 - accuracy: 0.7829 - val_loss: 0.7527 - val_accuracy: 0.7390\n",
      "Epoch 46/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6554 - accuracy: 0.7849 - val_loss: 0.7515 - val_accuracy: 0.7310\n",
      "Epoch 47/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6473 - accuracy: 0.7865 - val_loss: 0.7404 - val_accuracy: 0.7370\n",
      "Epoch 48/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6371 - accuracy: 0.7911 - val_loss: 0.7364 - val_accuracy: 0.7240\n",
      "Epoch 49/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6289 - accuracy: 0.7951 - val_loss: 0.7301 - val_accuracy: 0.7490\n",
      "Epoch 50/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6210 - accuracy: 0.7944 - val_loss: 0.7232 - val_accuracy: 0.7400\n",
      "Epoch 51/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6129 - accuracy: 0.7981 - val_loss: 0.7166 - val_accuracy: 0.7500\n",
      "Epoch 52/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6055 - accuracy: 0.7984 - val_loss: 0.7129 - val_accuracy: 0.7460\n",
      "Epoch 53/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5983 - accuracy: 0.8033 - val_loss: 0.7119 - val_accuracy: 0.7450\n",
      "Epoch 54/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5913 - accuracy: 0.8051 - val_loss: 0.7071 - val_accuracy: 0.7530\n",
      "Epoch 55/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5847 - accuracy: 0.8048 - val_loss: 0.7045 - val_accuracy: 0.7520\n",
      "Epoch 56/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5779 - accuracy: 0.8087 - val_loss: 0.6992 - val_accuracy: 0.7440\n",
      "Epoch 57/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5715 - accuracy: 0.8101 - val_loss: 0.6965 - val_accuracy: 0.7520\n",
      "Epoch 58/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5657 - accuracy: 0.8117 - val_loss: 0.6924 - val_accuracy: 0.7500\n",
      "Epoch 59/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5596 - accuracy: 0.8120 - val_loss: 0.6942 - val_accuracy: 0.7520\n",
      "Epoch 60/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5543 - accuracy: 0.8149 - val_loss: 0.6885 - val_accuracy: 0.7570\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "final_model = model.fit(train_final, label_train_final, epochs=60, batch_size=256, validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we did before, get our results using `model.evaluate()` on the appropriate variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 1ms/step - loss: 0.5492 - accuracy: 0.8167\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 1ms/step - loss: 0.6962 - accuracy: 0.7547\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5491539835929871, 0.8166666626930237]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train  # Expected Output: [0.58606486314137773, 0.79826666669845581]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6961879134178162, 0.7546666860580444]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # [0.74768974288304646, 0.71333333365122475]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument `kernel_regulizers.l2` and adding a value for the regularization parameter lambda between parentheses.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did before.\n",
    "* In our two hidden layers (but not our output layer), add in the parameter `kernel_regularizer=regularizers.l2(0.005)` to add L2 regularization to each hidden layer.  \n",
    "* Compile the model with the same hyperparameters as we did before. \n",
    "* Fit the model with the same hyperparameters as we did before, but this time for `120` epochs.\n",
    "* Store the fitted model that the `.fit` call returns inside a variable called `L2_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 1s 16ms/step - loss: 2.5904 - accuracy: 0.1720 - val_loss: 2.5714 - val_accuracy: 0.1920\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.5534 - accuracy: 0.2300 - val_loss: 2.5394 - val_accuracy: 0.2380\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.5187 - accuracy: 0.2677 - val_loss: 2.5068 - val_accuracy: 0.2670\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.4818 - accuracy: 0.3036 - val_loss: 2.4711 - val_accuracy: 0.3000\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.4423 - accuracy: 0.3316 - val_loss: 2.4326 - val_accuracy: 0.3360\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.4007 - accuracy: 0.3563 - val_loss: 2.3921 - val_accuracy: 0.3560\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.3573 - accuracy: 0.3752 - val_loss: 2.3492 - val_accuracy: 0.3720\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.3120 - accuracy: 0.3932 - val_loss: 2.3052 - val_accuracy: 0.3900\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.2659 - accuracy: 0.4075 - val_loss: 2.2604 - val_accuracy: 0.4010\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.2191 - accuracy: 0.4235 - val_loss: 2.2139 - val_accuracy: 0.4210\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.1725 - accuracy: 0.4401 - val_loss: 2.1686 - val_accuracy: 0.4480\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.1261 - accuracy: 0.4595 - val_loss: 2.1236 - val_accuracy: 0.4620\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.0801 - accuracy: 0.4823 - val_loss: 2.0803 - val_accuracy: 0.4730\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.0351 - accuracy: 0.5023 - val_loss: 2.0373 - val_accuracy: 0.4900\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.9919 - accuracy: 0.5217 - val_loss: 1.9947 - val_accuracy: 0.5090\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.9497 - accuracy: 0.5407 - val_loss: 1.9560 - val_accuracy: 0.5270\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.9096 - accuracy: 0.5609 - val_loss: 1.9164 - val_accuracy: 0.5580\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8707 - accuracy: 0.5835 - val_loss: 1.8813 - val_accuracy: 0.5750\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.8324 - accuracy: 0.5979 - val_loss: 1.8433 - val_accuracy: 0.5970\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7959 - accuracy: 0.6191 - val_loss: 1.8095 - val_accuracy: 0.6090\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7607 - accuracy: 0.6353 - val_loss: 1.7782 - val_accuracy: 0.6140\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7268 - accuracy: 0.6456 - val_loss: 1.7478 - val_accuracy: 0.6260\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.6941 - accuracy: 0.6540 - val_loss: 1.7165 - val_accuracy: 0.6380\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.6633 - accuracy: 0.6635 - val_loss: 1.6888 - val_accuracy: 0.6530\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6338 - accuracy: 0.6747 - val_loss: 1.6625 - val_accuracy: 0.6690\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6061 - accuracy: 0.6844 - val_loss: 1.6391 - val_accuracy: 0.6710\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5795 - accuracy: 0.6887 - val_loss: 1.6145 - val_accuracy: 0.6780\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.5544 - accuracy: 0.6980 - val_loss: 1.5901 - val_accuracy: 0.6840\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5306 - accuracy: 0.7044 - val_loss: 1.5696 - val_accuracy: 0.6850\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5082 - accuracy: 0.7103 - val_loss: 1.5493 - val_accuracy: 0.6870\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4869 - accuracy: 0.7148 - val_loss: 1.5286 - val_accuracy: 0.7000\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4668 - accuracy: 0.7189 - val_loss: 1.5128 - val_accuracy: 0.6980\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.4473 - accuracy: 0.7232 - val_loss: 1.4957 - val_accuracy: 0.6980\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4293 - accuracy: 0.7261 - val_loss: 1.4805 - val_accuracy: 0.7070\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.4121 - accuracy: 0.7313 - val_loss: 1.4666 - val_accuracy: 0.6950\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3958 - accuracy: 0.7348 - val_loss: 1.4518 - val_accuracy: 0.7070\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3803 - accuracy: 0.7379 - val_loss: 1.4386 - val_accuracy: 0.7070\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3656 - accuracy: 0.7396 - val_loss: 1.4256 - val_accuracy: 0.7090\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3517 - accuracy: 0.7441 - val_loss: 1.4135 - val_accuracy: 0.7120\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3380 - accuracy: 0.7479 - val_loss: 1.4028 - val_accuracy: 0.7150\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3251 - accuracy: 0.7523 - val_loss: 1.3889 - val_accuracy: 0.7230\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3128 - accuracy: 0.7545 - val_loss: 1.3811 - val_accuracy: 0.7220\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3010 - accuracy: 0.7577 - val_loss: 1.3701 - val_accuracy: 0.7140\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2897 - accuracy: 0.7585 - val_loss: 1.3605 - val_accuracy: 0.7210\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2786 - accuracy: 0.7611 - val_loss: 1.3510 - val_accuracy: 0.7320\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2678 - accuracy: 0.7653 - val_loss: 1.3407 - val_accuracy: 0.7250\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2577 - accuracy: 0.7644 - val_loss: 1.3341 - val_accuracy: 0.7250\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2477 - accuracy: 0.7665 - val_loss: 1.3253 - val_accuracy: 0.7370\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2379 - accuracy: 0.7691 - val_loss: 1.3170 - val_accuracy: 0.7370\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2286 - accuracy: 0.7725 - val_loss: 1.3112 - val_accuracy: 0.7320\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2195 - accuracy: 0.7751 - val_loss: 1.3029 - val_accuracy: 0.7370\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2108 - accuracy: 0.7787 - val_loss: 1.2945 - val_accuracy: 0.7390\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2020 - accuracy: 0.7795 - val_loss: 1.2910 - val_accuracy: 0.7390\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1936 - accuracy: 0.7799 - val_loss: 1.2829 - val_accuracy: 0.7390\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1855 - accuracy: 0.7823 - val_loss: 1.2793 - val_accuracy: 0.7340\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1775 - accuracy: 0.7871 - val_loss: 1.2705 - val_accuracy: 0.7390\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1696 - accuracy: 0.7880 - val_loss: 1.2654 - val_accuracy: 0.7380\n",
      "Epoch 58/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1620 - accuracy: 0.7916 - val_loss: 1.2610 - val_accuracy: 0.7410\n",
      "Epoch 59/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1548 - accuracy: 0.7921 - val_loss: 1.2548 - val_accuracy: 0.7410\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1477 - accuracy: 0.7955 - val_loss: 1.2490 - val_accuracy: 0.7400\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1400 - accuracy: 0.7969 - val_loss: 1.2425 - val_accuracy: 0.7450\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1334 - accuracy: 0.7979 - val_loss: 1.2362 - val_accuracy: 0.7440\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1264 - accuracy: 0.8017 - val_loss: 1.2316 - val_accuracy: 0.7400\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1197 - accuracy: 0.8036 - val_loss: 1.2270 - val_accuracy: 0.7420\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1128 - accuracy: 0.8060 - val_loss: 1.2257 - val_accuracy: 0.7440\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1065 - accuracy: 0.8057 - val_loss: 1.2202 - val_accuracy: 0.7400\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1003 - accuracy: 0.8099 - val_loss: 1.2145 - val_accuracy: 0.7440\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0936 - accuracy: 0.8083 - val_loss: 1.2083 - val_accuracy: 0.7450\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0872 - accuracy: 0.8115 - val_loss: 1.2042 - val_accuracy: 0.7520\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0817 - accuracy: 0.8141 - val_loss: 1.2004 - val_accuracy: 0.7570\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0753 - accuracy: 0.8148 - val_loss: 1.1989 - val_accuracy: 0.7440\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0697 - accuracy: 0.8129 - val_loss: 1.1934 - val_accuracy: 0.7480\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0638 - accuracy: 0.8167 - val_loss: 1.1881 - val_accuracy: 0.7450\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0582 - accuracy: 0.8180 - val_loss: 1.1854 - val_accuracy: 0.7550\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0523 - accuracy: 0.8201 - val_loss: 1.1807 - val_accuracy: 0.7540\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0473 - accuracy: 0.8235 - val_loss: 1.1808 - val_accuracy: 0.7490\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0416 - accuracy: 0.8237 - val_loss: 1.1749 - val_accuracy: 0.7560\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0362 - accuracy: 0.8261 - val_loss: 1.1709 - val_accuracy: 0.7520\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0307 - accuracy: 0.8267 - val_loss: 1.1666 - val_accuracy: 0.7580\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0258 - accuracy: 0.8269 - val_loss: 1.1645 - val_accuracy: 0.7520\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0201 - accuracy: 0.8291 - val_loss: 1.1626 - val_accuracy: 0.7570\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0149 - accuracy: 0.8319 - val_loss: 1.1571 - val_accuracy: 0.7600\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0095 - accuracy: 0.8331 - val_loss: 1.1572 - val_accuracy: 0.7530\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0051 - accuracy: 0.8347 - val_loss: 1.1520 - val_accuracy: 0.7560\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0001 - accuracy: 0.8344 - val_loss: 1.1477 - val_accuracy: 0.7530\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9951 - accuracy: 0.8360 - val_loss: 1.1433 - val_accuracy: 0.7620\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9902 - accuracy: 0.8369 - val_loss: 1.1436 - val_accuracy: 0.7560\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9854 - accuracy: 0.8385 - val_loss: 1.1374 - val_accuracy: 0.7580\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9807 - accuracy: 0.8405 - val_loss: 1.1389 - val_accuracy: 0.7590\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9762 - accuracy: 0.8413 - val_loss: 1.1347 - val_accuracy: 0.7570\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9715 - accuracy: 0.8432 - val_loss: 1.1298 - val_accuracy: 0.7600\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9667 - accuracy: 0.8451 - val_loss: 1.1299 - val_accuracy: 0.7610\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9624 - accuracy: 0.8476 - val_loss: 1.1272 - val_accuracy: 0.7580\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9581 - accuracy: 0.8485 - val_loss: 1.1264 - val_accuracy: 0.7540\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9535 - accuracy: 0.8489 - val_loss: 1.1196 - val_accuracy: 0.7590\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9488 - accuracy: 0.8508 - val_loss: 1.1170 - val_accuracy: 0.7610\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9448 - accuracy: 0.8513 - val_loss: 1.1159 - val_accuracy: 0.7640\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9403 - accuracy: 0.8532 - val_loss: 1.1143 - val_accuracy: 0.7590\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9362 - accuracy: 0.8543 - val_loss: 1.1080 - val_accuracy: 0.7620\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9316 - accuracy: 0.8576 - val_loss: 1.1064 - val_accuracy: 0.7600\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9274 - accuracy: 0.8587 - val_loss: 1.1038 - val_accuracy: 0.7620\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9232 - accuracy: 0.8587 - val_loss: 1.1065 - val_accuracy: 0.7550\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9194 - accuracy: 0.8600 - val_loss: 1.0983 - val_accuracy: 0.7650\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9152 - accuracy: 0.8619 - val_loss: 1.1022 - val_accuracy: 0.7610\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9115 - accuracy: 0.8605 - val_loss: 1.0992 - val_accuracy: 0.7590\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9072 - accuracy: 0.8633 - val_loss: 1.0925 - val_accuracy: 0.7690\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9034 - accuracy: 0.8633 - val_loss: 1.0927 - val_accuracy: 0.7630\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.8995 - accuracy: 0.8649 - val_loss: 1.0915 - val_accuracy: 0.7580\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8954 - accuracy: 0.8681 - val_loss: 1.0864 - val_accuracy: 0.7600\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8917 - accuracy: 0.8663 - val_loss: 1.0859 - val_accuracy: 0.7630\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.8875 - accuracy: 0.8687 - val_loss: 1.0814 - val_accuracy: 0.7580\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8843 - accuracy: 0.8687 - val_loss: 1.0802 - val_accuracy: 0.7610\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8803 - accuracy: 0.8703 - val_loss: 1.0781 - val_accuracy: 0.7630\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.8764 - accuracy: 0.8733 - val_loss: 1.0766 - val_accuracy: 0.7640\n",
      "Epoch 115/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.8731 - accuracy: 0.8713 - val_loss: 1.0731 - val_accuracy: 0.7600\n",
      "Epoch 116/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8691 - accuracy: 0.8743 - val_loss: 1.0760 - val_accuracy: 0.7580\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8654 - accuracy: 0.8743 - val_loss: 1.0725 - val_accuracy: 0.7630\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8620 - accuracy: 0.8757 - val_loss: 1.0710 - val_accuracy: 0.7620\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.8583 - accuracy: 0.8756 - val_loss: 1.0671 - val_accuracy: 0.7580\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.8549 - accuracy: 0.8776 - val_loss: 1.0638 - val_accuracy: 0.7680\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "L2_model = model.fit(train_final, label_train_final, epochs=120,batch_size=256, validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how regularization has affected our model results.  \n",
    "\n",
    "Run the cell below to get the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs).\n",
    "\n",
    "Run the cell below to visualize our training and validation accuracy both with and without L2 regularization, so that we can compare them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['accuracy'] \n",
    "val_acc_values = L2_model_dict['val_accuracy']\n",
    "model_acc = model_val_dict['accuracy']\n",
    "model_val_acc = model_val_dict['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did above, but this time, set the `kernel_regularizer` to `regularizers.l1(0.005)` inside both hidden layers. \n",
    "* Compile and fit the model exactly as we did for our L2 Regularization experiment (`120` epochs) \n",
    "* Store the fitted model that the `.fit` call returns inside a variable called `L1_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 1s 16ms/step - loss: 3.3048 - accuracy: 0.1685 - val_loss: 3.2951 - val_accuracy: 0.1820\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 3.2749 - accuracy: 0.2052 - val_loss: 3.2686 - val_accuracy: 0.2220\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 3.2474 - accuracy: 0.2321 - val_loss: 3.2426 - val_accuracy: 0.2460\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.2189 - accuracy: 0.2537 - val_loss: 3.2153 - val_accuracy: 0.2550\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.1883 - accuracy: 0.2704 - val_loss: 3.1853 - val_accuracy: 0.2640\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 3.1547 - accuracy: 0.2885 - val_loss: 3.1516 - val_accuracy: 0.2740\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 3.1178 - accuracy: 0.3113 - val_loss: 3.1130 - val_accuracy: 0.3050\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 3.0764 - accuracy: 0.3361 - val_loss: 3.0696 - val_accuracy: 0.3390\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 3.0300 - accuracy: 0.3631 - val_loss: 3.0211 - val_accuracy: 0.3560\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.9795 - accuracy: 0.3771 - val_loss: 2.9695 - val_accuracy: 0.3730\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.9261 - accuracy: 0.4031 - val_loss: 2.9160 - val_accuracy: 0.3880\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.8711 - accuracy: 0.4195 - val_loss: 2.8616 - val_accuracy: 0.4140\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.8161 - accuracy: 0.4424 - val_loss: 2.8085 - val_accuracy: 0.4200\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.7616 - accuracy: 0.4551 - val_loss: 2.7548 - val_accuracy: 0.4470\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.7079 - accuracy: 0.4849 - val_loss: 2.7033 - val_accuracy: 0.4620\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.6558 - accuracy: 0.5048 - val_loss: 2.6541 - val_accuracy: 0.4730\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.6048 - accuracy: 0.5239 - val_loss: 2.6041 - val_accuracy: 0.4920\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.5558 - accuracy: 0.5387 - val_loss: 2.5566 - val_accuracy: 0.5130\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.5080 - accuracy: 0.5607 - val_loss: 2.5112 - val_accuracy: 0.5340\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.4622 - accuracy: 0.5769 - val_loss: 2.4673 - val_accuracy: 0.5490\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.4181 - accuracy: 0.5953 - val_loss: 2.4249 - val_accuracy: 0.5670\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.3761 - accuracy: 0.6095 - val_loss: 2.3859 - val_accuracy: 0.5760\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.3359 - accuracy: 0.6296 - val_loss: 2.3470 - val_accuracy: 0.5980\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.2975 - accuracy: 0.6429 - val_loss: 2.3121 - val_accuracy: 0.6090\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.2609 - accuracy: 0.6529 - val_loss: 2.2760 - val_accuracy: 0.6230\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.2259 - accuracy: 0.6604 - val_loss: 2.2451 - val_accuracy: 0.6270\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.1926 - accuracy: 0.6733 - val_loss: 2.2127 - val_accuracy: 0.6410\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.1610 - accuracy: 0.6796 - val_loss: 2.1834 - val_accuracy: 0.6470\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.1308 - accuracy: 0.6888 - val_loss: 2.1539 - val_accuracy: 0.6640\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.1018 - accuracy: 0.7005 - val_loss: 2.1276 - val_accuracy: 0.6560\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.0742 - accuracy: 0.7063 - val_loss: 2.1042 - val_accuracy: 0.6630\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 2.0479 - accuracy: 0.7131 - val_loss: 2.0807 - val_accuracy: 0.6660\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.0228 - accuracy: 0.7173 - val_loss: 2.0552 - val_accuracy: 0.6820\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.9986 - accuracy: 0.7248 - val_loss: 2.0339 - val_accuracy: 0.6900\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9757 - accuracy: 0.7276 - val_loss: 2.0129 - val_accuracy: 0.6870\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9533 - accuracy: 0.7308 - val_loss: 1.9926 - val_accuracy: 0.6940\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9321 - accuracy: 0.7376 - val_loss: 1.9736 - val_accuracy: 0.6930\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9113 - accuracy: 0.7403 - val_loss: 1.9546 - val_accuracy: 0.7020\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8916 - accuracy: 0.7431 - val_loss: 1.9372 - val_accuracy: 0.7060\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8723 - accuracy: 0.7452 - val_loss: 1.9183 - val_accuracy: 0.7070\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.8539 - accuracy: 0.7481 - val_loss: 1.9043 - val_accuracy: 0.7000\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.8360 - accuracy: 0.7508 - val_loss: 1.8866 - val_accuracy: 0.7070\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8186 - accuracy: 0.7537 - val_loss: 1.8721 - val_accuracy: 0.7060\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.8022 - accuracy: 0.7569 - val_loss: 1.8585 - val_accuracy: 0.7100\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.7864 - accuracy: 0.7597 - val_loss: 1.8445 - val_accuracy: 0.7130\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.7708 - accuracy: 0.7637 - val_loss: 1.8303 - val_accuracy: 0.7190\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.7557 - accuracy: 0.7643 - val_loss: 1.8178 - val_accuracy: 0.7150\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.7408 - accuracy: 0.7649 - val_loss: 1.8062 - val_accuracy: 0.7260\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.7268 - accuracy: 0.7697 - val_loss: 1.7926 - val_accuracy: 0.7220\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.7128 - accuracy: 0.7705 - val_loss: 1.7813 - val_accuracy: 0.7260\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.6997 - accuracy: 0.7716 - val_loss: 1.7688 - val_accuracy: 0.7280\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.6860 - accuracy: 0.7744 - val_loss: 1.7574 - val_accuracy: 0.7210\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.6731 - accuracy: 0.7740 - val_loss: 1.7455 - val_accuracy: 0.7300\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.6605 - accuracy: 0.7783 - val_loss: 1.7337 - val_accuracy: 0.7290\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.6480 - accuracy: 0.7800 - val_loss: 1.7260 - val_accuracy: 0.7250\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.6357 - accuracy: 0.7807 - val_loss: 1.7146 - val_accuracy: 0.7330\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6238 - accuracy: 0.7852 - val_loss: 1.7048 - val_accuracy: 0.7290\n",
      "Epoch 58/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.6118 - accuracy: 0.7863 - val_loss: 1.6958 - val_accuracy: 0.7290\n",
      "Epoch 59/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6008 - accuracy: 0.7843 - val_loss: 1.6834 - val_accuracy: 0.7340\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5895 - accuracy: 0.7895 - val_loss: 1.6749 - val_accuracy: 0.7340\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5785 - accuracy: 0.7892 - val_loss: 1.6646 - val_accuracy: 0.7340\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5676 - accuracy: 0.7939 - val_loss: 1.6577 - val_accuracy: 0.7420\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5572 - accuracy: 0.7939 - val_loss: 1.6485 - val_accuracy: 0.7340\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5465 - accuracy: 0.7948 - val_loss: 1.6402 - val_accuracy: 0.7340\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5359 - accuracy: 0.7957 - val_loss: 1.6311 - val_accuracy: 0.7370\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.5263 - accuracy: 0.7979 - val_loss: 1.6230 - val_accuracy: 0.7380\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5162 - accuracy: 0.8001 - val_loss: 1.6152 - val_accuracy: 0.7370\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5064 - accuracy: 0.8000 - val_loss: 1.6081 - val_accuracy: 0.7360\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4970 - accuracy: 0.8041 - val_loss: 1.5987 - val_accuracy: 0.7370\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4870 - accuracy: 0.8043 - val_loss: 1.5898 - val_accuracy: 0.7360\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4776 - accuracy: 0.8057 - val_loss: 1.5831 - val_accuracy: 0.7390\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4683 - accuracy: 0.8049 - val_loss: 1.5741 - val_accuracy: 0.7400\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4591 - accuracy: 0.8091 - val_loss: 1.5682 - val_accuracy: 0.7430\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4503 - accuracy: 0.8087 - val_loss: 1.5597 - val_accuracy: 0.7400\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4411 - accuracy: 0.8121 - val_loss: 1.5524 - val_accuracy: 0.7400\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4321 - accuracy: 0.8127 - val_loss: 1.5445 - val_accuracy: 0.7400\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4237 - accuracy: 0.8120 - val_loss: 1.5379 - val_accuracy: 0.7440\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4148 - accuracy: 0.8124 - val_loss: 1.5289 - val_accuracy: 0.7390\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4067 - accuracy: 0.8156 - val_loss: 1.5235 - val_accuracy: 0.7420\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3983 - accuracy: 0.8164 - val_loss: 1.5159 - val_accuracy: 0.7420\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3897 - accuracy: 0.8179 - val_loss: 1.5093 - val_accuracy: 0.7440\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3812 - accuracy: 0.8171 - val_loss: 1.5038 - val_accuracy: 0.7440\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3734 - accuracy: 0.8199 - val_loss: 1.4970 - val_accuracy: 0.7420\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3653 - accuracy: 0.8200 - val_loss: 1.4912 - val_accuracy: 0.7430\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3570 - accuracy: 0.8228 - val_loss: 1.4836 - val_accuracy: 0.7410\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3492 - accuracy: 0.8217 - val_loss: 1.4783 - val_accuracy: 0.7450\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3419 - accuracy: 0.8215 - val_loss: 1.4701 - val_accuracy: 0.7430\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3343 - accuracy: 0.8252 - val_loss: 1.4638 - val_accuracy: 0.7450\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3264 - accuracy: 0.8268 - val_loss: 1.4599 - val_accuracy: 0.7480\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3189 - accuracy: 0.8237 - val_loss: 1.4512 - val_accuracy: 0.7480\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3119 - accuracy: 0.8279 - val_loss: 1.4460 - val_accuracy: 0.7450\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3044 - accuracy: 0.8304 - val_loss: 1.4391 - val_accuracy: 0.7500\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2976 - accuracy: 0.8271 - val_loss: 1.4367 - val_accuracy: 0.7420\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2901 - accuracy: 0.8311 - val_loss: 1.4282 - val_accuracy: 0.7500\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2833 - accuracy: 0.8300 - val_loss: 1.4222 - val_accuracy: 0.7470\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2763 - accuracy: 0.8303 - val_loss: 1.4189 - val_accuracy: 0.7490\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2693 - accuracy: 0.8337 - val_loss: 1.4112 - val_accuracy: 0.7510\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2625 - accuracy: 0.8323 - val_loss: 1.4079 - val_accuracy: 0.7450\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2560 - accuracy: 0.8329 - val_loss: 1.4020 - val_accuracy: 0.7500\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2488 - accuracy: 0.8351 - val_loss: 1.3957 - val_accuracy: 0.7550\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2428 - accuracy: 0.8365 - val_loss: 1.3881 - val_accuracy: 0.7500\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2357 - accuracy: 0.8385 - val_loss: 1.3862 - val_accuracy: 0.7530\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2293 - accuracy: 0.8387 - val_loss: 1.3790 - val_accuracy: 0.7530\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2230 - accuracy: 0.8393 - val_loss: 1.3749 - val_accuracy: 0.7530\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2166 - accuracy: 0.8373 - val_loss: 1.3689 - val_accuracy: 0.7490\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2104 - accuracy: 0.8409 - val_loss: 1.3653 - val_accuracy: 0.7520\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2042 - accuracy: 0.8393 - val_loss: 1.3586 - val_accuracy: 0.7530\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1982 - accuracy: 0.8420 - val_loss: 1.3561 - val_accuracy: 0.7560\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1920 - accuracy: 0.8428 - val_loss: 1.3498 - val_accuracy: 0.7580\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1860 - accuracy: 0.8427 - val_loss: 1.3442 - val_accuracy: 0.7500\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1801 - accuracy: 0.8468 - val_loss: 1.3392 - val_accuracy: 0.7530\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1741 - accuracy: 0.8444 - val_loss: 1.3333 - val_accuracy: 0.7580\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1686 - accuracy: 0.8445 - val_loss: 1.3279 - val_accuracy: 0.7560\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1623 - accuracy: 0.8497 - val_loss: 1.3238 - val_accuracy: 0.7560\n",
      "Epoch 115/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1566 - accuracy: 0.8475 - val_loss: 1.3212 - val_accuracy: 0.7520\n",
      "Epoch 116/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1512 - accuracy: 0.8520 - val_loss: 1.3163 - val_accuracy: 0.7550\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1454 - accuracy: 0.8507 - val_loss: 1.3162 - val_accuracy: 0.7600\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1403 - accuracy: 0.8491 - val_loss: 1.3075 - val_accuracy: 0.7570\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1343 - accuracy: 0.8516 - val_loss: 1.3047 - val_accuracy: 0.7500\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1291 - accuracy: 0.8520 - val_loss: 1.2983 - val_accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "L1_model = model.fit(train_final, label_train_final, epochs=120,batch_size=256, validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to get and visualize the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABAP0lEQVR4nO3deXgUVfbw8e/pzkYIhk3WsCqCIBAwiuCGig5uKIojiAvquPvOqOPMgPNTcdxnHHVmXHBDRVHclUFwgTGCEpUdAQERAgFkCxgI2ZPz/lGV2Gk6SQfS6XT6fJ4nT7qrblWf6qVO1b23bomqYowxJnp5wh2AMcaY8LJEYIwxUc4SgTHGRDlLBMYYE+UsERhjTJSzRGCMMVHOEkENRGSWiFxV12UbMhEZJyJf+TzPFZHuwZQ9iNdqFO9ZQycik0Tk7mrmTxSR1+szpvp2qNtY03t4COsN+28gJpwvHioikuvzNBEoBErd5zeo6tRg16WqZ4eibG2JSEvgVeAUYD/wpKr+PVSv50tVk+piPSIyEThSVS/3WXfI3jPzK1W9sfyxiAwFXlfVlINdn4go0ENV1/lNbw88B6QB7YFuqpp5sK/TkPi+hwerof4GGuUZgaomlf8Bm4DzfaZVJAERiaRE+CcgAefH1Qf4OrzhmOpE2HerLpUBnwAX13bBhvyeiYg33DGEUqNMBFURkaEisllE/iIi24CXRaSFiMwQkZ0issd9nOKzTLqI/M59PE5EvhKRx9yyG0Tk7IMs201E5orIPhGZLSJP13DaWgLsUNU8Vd2jqtUmAvc09jG/aR+JyB3u4/Ei8pP7+qtEZGQ161IROdJ93EpEpovIXhH5DjjCr+y/RCTLnb9IRE52pw8H7gIudaualgV4zzwi8n8islFEdojIFBFJdud1deO4SkQ2icguEflrNTGfKyJL3Diy3CMx3/knich8EfnFnT/Ond5ERP7pxpDjfoZNyr87fuvIFJFh7uOJIvKuiLwuInuBcSJyvIhkuK/xs4g8JSJxPsv3EZHPRWS3iGwXkbtEpJ2I5IlIK59yx7rfz1i/108QkXwRae0+/z8RKRGRw9znD4jIk+7jV9znTYFZQAf3c8gVkQ7uKuPc93yfiKwUkbSq3t+qqOp2VX0GWBBMefc9/IuILAf2i0iMiJzg89ksE+cMprx8lb+bmj6jAK/9johscz/nuSLSx2feKyLyrIjMFJH9wGnl76E7/78+71+uiJT5fIcaxG+gNqIqEbjaAS2BLsD1OO/By+7zzkA+8FQ1yw8C1gCtgb8DL4mIHETZN4DvgFbAROCKGuL+DhgjItfUUK7cGzhfOAEQkRbAWcA0d/5PwMlAMnAf8Lo4p/U1eRoowDkzucb987UASMV5j98A3hGRBFX9BHgIeMs9M+sfYN3j3L/TgO5AEgd+FicBPYEzgHtE5Ogq4twPXAk0B84FbhKRCwFEpDPOzvA/wOFuvEvd5R4DjgWGuNvwZ5yj3GBcALzrvuZUnOrI23E+/8FuzDe7MTQDZuMcPXcAjgTmqOo2IB34rc96LwemqWqx74upagHO+32qO+kUYCNwos/zL/2W2Q+cDWz1OUve6s4egfP9aA5Mp/rfQV0ag/MZNQfaAh8DD+C8/3cC74nI4W7Z2v5uqjML6AG0ARbjfGa+LgMeBJoBldrBVPV8n1qHUcA2YI47u6H8BoKnqo36D8gEhrmPhwJFQEI15VOBPT7P04HfuY/HAet85iUCCrSrTVmchFMCJPrMfx2n3jZQTEcCP+P8sNcCV7vT493tSQ6wjOBUi53iPr8O+F81270UuMAn9q985qkbgxcoBnr5zHvIt2yA9e4B+ruPJ/pvo997Nge42WdeT/f1YoCubhwpPvO/A0YH+T14EnjCfTwB+CBAGQ/OgUD/APOGApur+W5NBObWEMNt5a+Ls/NbUkW5S4Gv3cdenJ3M8VWUvR/4t/sebQP+ADyCU42YD7R2y70CPFDNtkwEZvs87w3kV7MtilPXXdX8GLdM1xrek0zgGp/nfwFe8yvzKXAVNfxugvyMqvqNNXfjTfZ5v6b4lal4D32mHQXsAE5u6L+B6v6i8YxgpzpHUgCISKKIPOeeiu0F5gLNpeo6wW3lD1Q1z31YVWNqVWU7ALt9pgFkVRPztcDnqjoX+A1wv4hcDZyAszPJ8V9AnW/JNJwdDjhHN77tI1eKyFL39PsX4BicI9fqHI7zhfSNdaNvARH5o4j84J5u/4JzxlHTest18FvfRvf12vpM2+bzOI8q3nsRGSQiX7hVKjnAjT5xdMI5I/LXGmcHGmheMCp9hiJylDhVjdvc79ZDQcQA8BHQW5yeWmcCOar6XRVlv8TZAQ4Evgc+xzlDOAHnQGRXLeL3f28TpH7q7X3fty7AJeXfS/c7dBLOGWhtfzdVEhGviDwiTvXoXpyEAZW/q9Wu262y+Qi4W1Xn+UxvEL+B2ojGROA/3OofcbLuIFU9DOeoG5wj6lD5GWgpIok+0zpVUz4G50gIVd0ADMepanoR+Fs1y70JjBKRLjjVVO8BuM9fAG4FWqlqc2AFNW/zTjcO31g7lz9w60L/glOt0cJdb47Pemsa6nYrzo7Ad90lwPYalgvkDZzqjU6qmgxM8okjC7+2DdcunGqvQPP245zVARWNh4f7lfHfvmeB1Ti9aw7DqR+uKQbcA5W3gbE4VR+vBSrnmo/z/R0JfKmqq3Det3PxqxaqJs5w840nC+eMoLnPX1NVfYSafzfBfEblLsOpyhuGs6PuWr5YFXFVIiIenO/YF6r6nM/0hvQbCFo0JgJ/zXBOoX8Rp4vmvaF+QVXdCCwEJopInIgMBs6vZpH3cer7L3S/3HuBZTg7kiq/WKq6BGfn/SLwqar+4s5q6i63E8A9uzgmiLhL3VgmumdSvXFO2cs1w/nS7gRiROQe4DCf+duBru6PKJA3gdvFaRBM4tf61JKaYgugGc7RY4GIHI/zwy83FRgmIr8Vp3GylYikqmoZMBl4XEQ6uEeNg0UkHqdKLkGcRuhY4P9wquZqimEvkCsivYCbfObNANqJyG0iEi8izURkkM/8KThVdCNwqj8Cco+OFwG38OuOfz5wA1Ungu1Aq/JGyEMQJ06DdfmfF5xGbH59b+Ld58F6HThfRH7jvv8J4jQCpwTxu6nNZ9QMp1t5Nk7yeKgWMYLTdtAUpyrOf70N5TcQNEsETt1xE5yjwW9wGu/qw1icBsRsnIaxt3C+mAdQ1QycHdm9OPWNnwIzcbrovSkiA6p5nTdxjnre8FnfKuCfQAbOF7MvwXdHvRXnVHQbTp3pyz7zPsVpgFuLc0pbQOXT63fc/9kisjjAuifjHP3OBTa4y/+/IOPydzPwNxHZB9yDc4QNgKpuAs7BORvcjdM+Ut5wdydOFcsCd96jgMetfrsZJ6luwTn6rNRDJYA7cT63fThnYG/5xLAPp9rnfJz38kecBsLy+V/jNFIv1pr74X8JxOLUF5c/b4bzPh5AVVfjfC/Wu9UvHQKVC8JKnIOo8r+r3en5QPm1PKvd50FR1SycI/W7cHamWThdp8v3VVX+bmr5GU3B+Y5uAVbh/PZrYwxO9dse+bXn0Fga1m8gaOI2OJgwE5G3gNWqGvIzEhMZROR/wBuq+mK4Y2mo7HdTN+yMIExE5DgROUKcfsPDcY6CPgxzWKaBEJHjcBqA36qpbDSx301oNNgr+aJAO5z69lY4p683uXX6JsqJyKvAhcAf3Cok8yv73YSAVQ0ZY0yUs6ohY4yJchFXNdS6dWvt2rVruMMwxpiIsmjRol2qGvC6iohLBF27dmXhwoXhDsMYYyKKiGysap5VDRljTJSzRGCMMVHOEoExxkQ5SwTGGBPlLBEYY0yUs0RgjDFRzhKBMcZEuYi7jsAYYxqLn/f9zJsr3qRlk5b0bNWTow8/muYJzQEoLStlwdYFLN22lNyiXPYX7WdIpyGcecSZdR6HJQJjjAmxkrISJi2cxGvLX6Nfm36c1u00Fm1dxDMLn6GgpKBS2U6HdeKoVkexdNtSsvOzK80bf+L4kCSCiBt0Li0tTe3KYmNMQ5Kdl83TC57m2YXP0rJJS87qfhaDOw3GIx5yi3L5Z8Y/WbFjBX3b9GVTziZyCnPwiIcr+l3BXSffhSCsyV7Dqp2rWL59Oat3rab34b05+8izOanzSTRPaE5ibCJeT1W3Uq+ZiCxS1bSA8ywRGGPMgX7Y+QO/FPzCMW2OISkuiZU7V/LfNf9l2fZlbN+/nR37dwDgFS/r96xnf/F+zj7ybEq1lLkb51Y60u+S3IXHf/M4I3uNpEzLWLZ9Gc0TmtO9Rfd6257qEoFVDRljGj1VZWPORuZnzefbzd+SU5hDSVkJBSUFZOdnszt/N+2T2jOk0xDaJbXj1WWvMj9rfsXyLZu0ZHf+bgCOaHEE7Zu1p2ernng9XkrLShnSaQi3Hn8rx7Rxbv1dUFLA2uy1eMRDjCeGbs27ER/j3D7ZK14Gth9Y/29CNSwRGGMi1tZ9W9lftJ/uLbpXqjbZ+MtG5m2ax1ebvmLFjhWs3LmSXwp+AaBpbFNaJ7YmxhNDfEw8LZu0pEtyFzJ/yWRi+kQUpUfLHvzzrH9yZMsj+X7796zfs55BKYM476jz6NCs5ls8J8Qk0K9tv1Btdp2zRGCMaRC27N3CvqJ99Grdq2La8u3L+WrTV3Ro1oEuyV2Ij4mnoKSA9XvWM3nJZD5Z9wmKEu+Np1uLbuQV55Gdl83+4v0AJMcn079df0b3GU3ftn0Z0mkIx7Q5hhhP4F1fTkEOm3I2cUybYxARAEb0HBH6jQ8zSwTGmHqRU5BD1t4sjm59dMXRe1ZOFs8teo4Za2ewbPsyAFLbpfLb3r8lfWM6n/30WZXr69isI/93yv/RrXk3Vu1cxfpf1pMUl0SrJq3o1rwbp3Q5hWPaHFOrBtbkhGT6JvQ9tA2NQJYIjDGHRFUpKi2qqAP3nf7dlu94ZuEzzN04l8xfMgFo27Qtv+3zW/YW7mXq91NRVU7qfBKPDnuUJjFNmLJ8Cnf97y7aJbXjodMfYkzfMWTnZbMxZyMlZSUkxCTQIqEFgzsNrvLI3tSO9RoyxgRtT/4e1mSvoaSshP1F+5m9fjbvr36f9XvW0zqxNZ2TO9MioQUJMQls3beVJduW0CyuGWf3OJvUtqm0S2rHjB9n8PHaj/GIh+sGXscdg++gS/MulV4nKyeLNk3bHJBczMGz7qPGmBrlFeexPXc7W/dtZdXOVazauYq84jzaJrUlMTaR2etnk56ZTqmWViwT64nl9G6nc0LKCWzL3camnE3sLdxLQUkBcd44Lu93OVf0u4Jm8c0qvda+wn0AB0w3oWPdR42JYgUlBeQU5FBUWkSrxFYkxiYCUFxazLrd65j6/VTe+P4NNvyyodJyibGJJMUlsXP/ThSlZ6ue/GnInzip80nEx8QT64kltV0qyQnJtY7JEkDDYonAmEZib+FeBCExNpGVO1fy4uIXeeP7Nw4YpiAhJoEyLaOotAgAj3g4s/uZXDfwOtoltaNdUjt6te5Fl+Zd8IiH0rJS9hbupUWTFuHYLFMPQpoIRGQ48C/AC7yoqo/4zU8GXgc6u7E8pqovhzImYyJdaVkp2/dvrxh2YMGWBTz69aO8/8P7KL9W9cZ54xjZayT92vajeUJzYj2x7M7fTXZ+Nh7xkBSXxOGJhzOi5wjaN2tf5et5PV5LAo1cyBKBiHiBp4Ezgc3AAhGZrqqrfIrdAqxS1fNF5HBgjYhMVdWiUMVlTKQoKSvhnZXv8NGajygsLaSkrISsnCxW71pNYWkh4Fwctb94P8nxyfxx8B9pm9SW3KJcDk88nNHHjKZVYqswb4WJBKE8IzgeWKeq6wFEZBpwAeCbCBRoJs6VG0nAbqAkhDEZ0yBty93GFxu+YE32GrzipbismNeXv86GXzbQsVlHWjZpidfjpUOzDpzZ/Uy6t+hOTmEO23K30SW5C9cOvJbD4g8L92aYCBXKRNARyPJ5vhkY5FfmKWA6sBVoBlyqqmX+KxKR64HrATp37hySYI2pD6rKnoI9bNm7he93fM/cjXOZu3EuP+z64YCyg1MG88RvnuD8nufjEbuHlAmdUCYCCTDNv6/qb4ClwOnAEcDnIjJPVfdWWkj1eeB5cLqP1n2oxtQdVWXG2hlMWT6FotIiVJWcwhy27N3C1n1byS/JryjbLK4ZJ3Y+kXGp4zi92+mktktFEEq1lDhvXBi3wkSTUCaCzUAnn+cpOEf+vq4GHlHnYoZ1IrIB6AV8F8K4jKlTW/ZuYcbaGcR4YhARXlryEvOz5tOhWQfaNG0DODv84zoeR8dmHUk5LIUOzTrQo2UP+rXtF3AIBC8HP+68MbUVykSwAOghIt2ALcBo4DK/MpuAM4B5ItIW6AmsD2FMxhyUdbvXsXrXajb+spHismIGdRxEnzZ9ePq7p3lw3oMVg5wBdGjWgefOe46rU68m1hsbxqiNCU7IEoGqlojIrcCnON1HJ6vqShG50Z0/CbgfeEVEvsepSvqLqu4KVUzG1NbSbUu554t7+O/a/1ZZ5sJeF/LAaQ9wWPxhFJQU0Dm5sw2NYCJKSK8jUNWZwEy/aZN8Hm8FzgplDMYEY3f+br7e9DWLf17M9zu+Z/v+7ezK28XqXatpntCcvw39G2cecSZdkp0xcb7Z/A2Lfl7EqV1ODck9ZI2pTzbWkIk6xaXFFaNY/pz7M4/Nf4znFj1HXnEegtCjVQ86NOtA68TW9G/bn1uPv5XmCc3DHbYxh8TGGjJRLbcol005m1jy8xI+WP0Bs9bNIq84r6JLpiCM7TeW3w34HQPaDyApLinMERtTvywRmEaloKSAZxc8y/S109mWu43tudvZU7CnYn77pPZc1f8quiR3IbcoF6/Hy1X9r6Jbi25hjNqY8LJEYCKSqpJXnEdOYQ578vdU1Oc/9NVDbMrZxMD2A+lzeB9O73o6nZI70SW5C0e1OooB7QfYxVnG+LFEYCLKml1reOKbJ3h9+euVumyWG9h+IJNHTOaM7meEIToTTTKyMkjPTGdo16EM7jQ46HkHs76DXWewLBGYBm9b7jY+Wv0R769+n89++ox4bzxj+o6hV6teNE9oTvOE5rRObE2bpm3o06aPHfGbCnW98yxfX6vEVtz2yW0UlRYR541jzpVzKtafkZXBGVPOqJj35PAnyc7LplViK7Lzsg+IJSMrgynLpvDy0pcpKSuptEx5Wf91+r5eXbBEYBqkfYX7eO+H93ht+Wt8seELFOWIFkdwzyn3cMvxt1RcsWuiW01H5YF2nr478/KdLXDAenzXDVTaWYsIZVpWcV+H9Mz0inVPTJ9IYWkhZVpGYUkht868ldKyUsoowyMe4r3xlZLDbZ/cRkFJQcUQ4uXLlGlZRdzpmekUlRZRqqWVXq+uWCIwDUZOQQ4z1s7g3R/eZdaPsygsLeSIFkdw9yl3M6r3KI5pcwzOQLUmEvnvWKvb8QYzraqj8vL5m3I2Vew8C0sKmZg+kYt7X8xtn9xGYUlhxY45xhODIJSUleD1eLkm9RoGtB9QsW6vx4sgzrhR7s7ao56K6V6Pl005m3h+0fMHrNsjHkrVSQJApeRQpmUVCaV8vYLg8bjLuGXL447zxlVsa/l7WFfsOgITVpv3bub9H95n+prpfLnxS0rKSujQrAOjjh7F6GNGc0LKCbbzr2PBVpfUtOMOdt3+VR/lO1DfapAlPy+pVDUy58o5AAdUsfiW8z0q94qX6wZeB3DA6xSXFlfaMZcvU07c8TF9d8Zej7eiXKD5CTEJ1cbjwcOw7sMCJh7fGDx4Kl7LPwn5LuN7FnGw1Vx2HYFpMApKCliwZQFzN87l4x8/JmNzBgC9D+/NHwf/kRE9R3BCygkRX88fyoa9Q3nNYKtLfI+2fXfc5TurK/tfCVDtMr47Nd+qj7JSZyesaMXRcUlZScX8otIipiybwvo96w+oYvEt539U/vLSlysdtVMG1w28jvV71jN7w2xn56/OHddQDjgjKF9WUcrKyiqtO9D2D+40mIfnPUxJWQmlWlopnjhvHBOHTmRwp8H0bdO3yvcpUHsAQN82fZmYPrEi7qLSIrLzsplw8oSQfHfsjMCEXHZeNjPWzuCD1R/w2U+fVQzDnNoulVFHj2JU71H0bN0zzFHWndo27NU2aQQ6Ug+0Ew60s96Us4kXFr/g7LiCPGoNdEQc642t8Wjb/8jaf1n/o+jarrv8qNx/u3znV3Vm4d94C1TbYFv+PgZq6A207oM5cwpUpi4biKs7I7BEYEIirziPZxc8y0drPuLrrK8p0zJSDkvhwp4XcuYRZ3JipxMj/jaKVTU6+h7JlVdZdE7ufMCOu6qj6EA78EA7K98j1WB3qOVHvzVVl/hXWfjXkfsnB99lKh21B6j68N8+/+0HqkxW/u9ToJ46Vc0/2KqtYL8HoTj7q8t1WyIw9erH7B8Z9c4olm9fTmq7VM4/6nzOP+p80jqkRVR9fzA9UgI1Ogba8QZbZ13TDryqnXGgnXCgnXV5YvKtLilftqqeLdUlId8Yy5fxr++v6SjZ/32uqfoqHP3sGwNLBKZe5BXnMW3FNG7/9HZiPDFMvWgqw48cHu6wgJp7pADV7oz8qxP8qyLgwB3zsO7D6N6ie0W5YI+iA5WrrnqmqkbXqnbWwVaX1LSzLX/PAi1zqDtl26nXPUsEJmQKSwqZs2EO76x6h/dWvce+on0M6jiIty95m87J9Xt/6WB6rATqkRKoF8t7q96r9oi5pqP/6l6npqPoQOUCdXH0rWKpKcGFYmdtIoslAlPn9hft545P7+CNFW+QW5TLYfGHMeroUVze73JO7Xpqvff6CVRHHKjHSqCqkUBVLL47/UB16L51/7W9MCmYHXOgcoHWbUywLBGYOrV131ZGvDmCxT8vZlzqOEb1HsUZ3c6o17ty+R/NPjzvYe7+4u5KvUaq67FS1dF2sH3By6tYbIdsIoVdR2DqhKry6U+f8rvpvyOnMIfpY6Zz3lHn1dvrB7qi1PfoP84bV3H0H6gvuG+PlDLcHX23YUwcOhE4sBdLdX3B7ajcNCZ2RmCCMmPtDO778j4Wbl1It+bd+HD0h/Rr2y/krxto5x+oq2SgKz0PdvAuqzs3jZGdEZiDVlBSwG2f3MZzi57jiBZH8ML5L3Bl/yuJ88Yd1PoC9b2v7SBf5VdwqmrF0X/5lZfPnvcsV/a/ssod+eBOgysG8apqRz+402BLACaqhDQRiMhw4F+AF3hRVR/xm/8nYKxPLEcDh6vq7lDGZYKzfs96fvvOb1n08yLGnzie+0+/nxjPwX9lAvW9D9SdsaZBvqoan6Y8gdS0I7cdvTGVhSwRiIgXeBo4E9gMLBCR6aq6qryMqv4D+Idb/nzgdksC4VdaVspT3z3FXf+7i1hPLB+N/ogRPUcc9Pr8R4P0HYkx0LgyvmPRgFP1Ex8TeNCt6o7+jTHBCeUZwfHAOlVdDyAi04ALgFVVlB8DvBnCeEwQ9uTv4dw3ziVjcwbn9DiHSedOolNyp4Nen3+3zhhPDFqqFWcE5YOFBdNn3qpxjAmNUCaCjkCWz/PNwKBABUUkERgO3FrF/OuB6wE6d67fi5Siiapy9UdXs3DrQl4b+Rpj+46t9ZAQ/m0AvmPCl48G6dv3vvwK3ap68diRvjGhF8pEEGgPUlUXpfOBr6uqFlLV54Hnwek1VDfhGX9PfPMEH635iCd+8wSX97u8xvJVjRHjP/5OjCcGyiDOG3fAkX1GVgavLnv1gO6agCUAY+pJKBPBZsC3TiEF2FpF2dFYtVBYfbP5G/4y+y+M7DWSPwz6Q43lA13JCxzQBlBaVlpp9M2D6cVjjAmtkF1HICIxwFrgDGALsAC4TFVX+pVLBjYAnVR1f03rtesI6t7qXas59ZVTaRrblMU3LKZ5QvMqy/o2/PqP/x7oql27AteYhiEs1xGoaomI3Ap8itN9dLKqrhSRG935k9yiI4HPgkkCpu79tPsnzphyBgCzxs6qMQn4N/yWlZZV9OX3Pfq3K3CNiRwhvY5AVWcCM/2mTfJ7/grwSijjMIGt2rmKc6aeQ0FJAelXpdd4l7D0zPQDGn6BSn35q+rdY4xpuOzK4iikqjyz4Bnu/PxOkuKS+Ozyz+jbtm+Nyw3tOpQ4b1xFw275Tt/68hsT2WysoSijqox9fyxvrniT4UcO5+ULXqZdUrugl7dxeIyJTDbWkKnw4uIXeXPFm9x76r3ce+q9QV8n4JsAJpw8IcRRGmPqkyWCKLJhzwbu+OwOTut6Gvecek+lJFDdrRz9h2a2XkDGNC6WCKJEmZYx7qNxCMLLF7xc6Q5igYZmhl8HgfMd+bOotIj0zHRLBMY0IpYIokBxaTE3f3wzczfOZfKIyXRp3qXSfN/eQIEGgfMf+bN8lE9jTONgiaCR25O/h1HvjOJ/G/7HX0/+K+NSxx1Qxrc3UKBB4MpvsG7XBRjTOFkiaMSy87I55ZVT+DH7R1698FWu7H9lwHK+wzxUNQic7fyNabwsETRS+cX5nP/m+fy0+yc+vfxTTut22gFl/BuIyweOq2oQOGNM42SJoBEqLStl7Ptj+WbzN7xzyTsHJIGMrIxKt4H07Qlkg8AZE30sETRC9315Hx+s/oB/Df8XF/e+uNK88h5CBSUFFXcA8+8JZDd7MSa6eGouYiLJoq2LeGjeQ1zV/yp+P+j3B8wv7yHkextI6wlkTHSzM4JGpKi0iHEfjaNtUlueHP5kwDL+PYSquw2kMSY6WCJoRB6Y+wArdqxgxpgZBwwn7dswbG0AxhhflggaiY2/bOThrx7min5XcO5R51ZMr6ph2MYLMsaUs0TQSDz13VOoKg+e/mDFtGAaho0xxhqLG4HcolxeWPwCo3qPolPyr7eJtoZhY0ww7IygEXhl6SvkFOZw+wm3V5puDcPGmGBYIohwZVrGv779FyeknMCglEGV5tnFYcaYYFgiiHAfr/2YdbvX8cBpD1RMCzR0hDHGVCWkiUBEhgP/ArzAi6r6SIAyQ4EngVhgl6qeGsqYGpOSshLu/uJuOid3rriCONC9BSwRGGOqE7LGYhHxAk8DZwO9gTEi0tuvTHPgGWCEqvYBLglVPI3Rk988ybLty3jiN08Q43Fyuv+9BdIz08MbpDGmwQtlr6HjgXWqul5Vi4BpwAV+ZS4D3lfVTQCquiOE8TQqmb9kcm/6vYzoOYKRvUYCztnAppxNxHhi8IrXeggZY4ISyqqhjkCWz/PNwCC/MkcBsSKSDjQD/qWqU/xXJCLXA9cDdO7cOSTBRhJV5ZaZtyAI/zn7P4hIpSohr8fLdQOvsx5CxpighDIRSIBpGuD1jwXOAJoAGSLyjaqurbSQ6vPA8wBpaWn+64g6n6//nJk/zuSfZ/2TzslOYvStEqIMOid3tiRgjAlKKBPBZqCTz/MUYGuAMrtUdT+wX0TmAv2BtZgqPTD3ATo268gtx91SMc33mgGrEjLG1EYoE8ECoIeIdAO2AKNx2gR8fQQ8JSIxQBxO1dETIYwp4s3bOI95m+bx5G+eJD4mvmK6XTNgjDlYIUsEqloiIrcCn+J0H52sqitF5EZ3/iRV/UFEPgGWA2U4XUxXhCqmxuChrx7i8MTDue7Y6w6YZ9cMGGMORkivI1DVmcBMv2mT/J7/A/hHKONoLBZtXcQn6z7hodMfIjE2ETjw4jFjjKktu7I4gjww7wGS45O5+bibAbt4zBhTN2z00QgxP2s+H67+kD8O/iPJCcmAXTxmjKkbdkYQAVSVP3/+Z9olteOOwXcAlS8eowzrKWSMOWiWCCLAR2s+4uusr3nuvOdoGtfULh4zxtQpSwQNXElZCRPmTKBX615cM+AawC4eM8bULUsEDdw7K99h9a7VvP/b9ysGlrOLx4wxdckSQQM3adEkjmhxBBf0+nW8Prt4zBhTlywRNGA/7PyBuRvn8uiwR/GIx244Y4wJCUsEDdjzi54n1hPLuNRxds2AMSZk7DqCBiq/OJ9Xl73KRUdfRJumbeyaAWNMyFgiaKDeXfUuewr2cMOxNwC/NhDbDWeMMXXNqoYaIFXl6QVPc1Sro4j3xvPwvIcZ2nWoNRAbY0IiqEQgIk2BfFUtE5GjgF7ALFUtDml0UeqN79/g2y3fMv7E8Qx7bVildoEJJ08Id3jGmEYm2KqhuUCCiHQE5gBXA6+EKqhotrdwL3d+fifHdTiOZvHNrF3AGBNywVYNiarmici1wH9U9e8isiSUgUWriekT2Z67nemjp1NSVmIXjhljQi7oRCAig4GxwLW1XNYEaeWOlfz7239z3cDrOK7jcQDWLmCMCblgd+a3AROAD9y7jHUHvghZVFHqgXkPkBibyINnPFgxzS4cM8aEWlCJQFW/BL4EEBEPzg3nfx/KwKLN+j3reXvl2/xx8B9pndja7jxmjKk3wfYaegO4ESgFFgHJIvK4e5tJUwcez3icGE8Mt51wm11FbIypV8H2GuqtqnuBC3HuQdwZuCJUQUWbnft3MnnJZK7odwUdmnWwq4iNMfUq2EQQKyKxOIngI/f6Aa1pIREZLiJrRGSdiIwPMH+oiOSIyFL3755aRd9I/Oe7/1BQUsCdQ+4E7CpiY0z9Crax+DkgE1gGzBWRLsDe6hYQES/wNHAmsBlYICLTVXWVX9F5qnperaJuRPYX7efpBU9zQa8L6NW6F2DDTBtj6lewjcX/Bv7tM2mjiJxWw2LHA+tUdT2AiEwDLgD8E0FUe3XZq+zO382fhvyp0nTrLWSMqS9BVQ2JSLKIPC4iC92/fwJNa1isI5Dl83yzO83fYBFZJiKzRKRPFa9/fflr79y5M5iQI0KZlvHEN08wqOMgBqcMJiMrg4fnPUxGVka4QzPGRJFgq4YmAyuA37rPrwBeBi6qZhkJMM2/XWEx0EVVc0XkHOBDoMcBC6k+DzwPkJaWVmPbRKSYsXYG63av44GLH+Cbzd9YTyFjTFgE21h8hKreq6rr3b/7gO41LLMZ6OTzPAXY6ltAVfeqaq77eCZOo3TrIGOKeI9nPE7n5M5c3Pti6ylkjAmbYBNBvoicVP5ERE4E8mtYZgHQQ0S6iUgcMBqY7ltARNqJiLiPj3fjyQ42+Ei2aOsivtz4Jb8//vfEeGKsp5AxJmyCrRq6EZgiIsnu8z3AVdUtoKolInIr8CngBSa7w1Pc6M6fBIwCbhKREpzEMlpVG03VT3VeWvISibGJ/G7g7wDrKWSMCR+pzX5XRA4Dp0pHRG5T1SdDFVhV0tLSdOHChfX9snVKVenyZBeO7XAsH1z6QbjDMcZEARFZpKppgebV6laVbp1++fUDdxxyZFFqxY4VZO3N4rwezuUT1lvIGBNOhzKUdKBeQSYIM9bOAOCcHufYuELGmLA7lJvXR0VdfijM+HEGaR3SaN+svfUWMsaEXbWJQET2icjeAH/7gA71FGOjsitvFxlZGfRv05+H5z1Mq8RW1lvIGBNW1VYNqWqz+gokWsz6cRaK8vr3r1fcivLJ4U+SnZdtvYWMMWFht5usZx//+DFJcUnkF+dXVAdl52Uz4eQJ4Q7NGBOlDqWNwNRSfnE+n6z7xC4eM8Y0KHZGUI/Gzx5PTmEOHZI6WHWQMabBqNUFZQ1BpF5Q9vlPn3PW62cR44lBVa2rqDGmXtXZBWXm4OzO3824j8bRpmkbVNW6ihpjGhRLBPVgwuwJ7Ny/k0eHPWptA8aYBsfaCEJsX+E+pn4/lSv7X8m41HH0bNXTBpYzxjQolghC7O2Vb7O/eD/XDrgWsFtQGmMaHqsaCrGXlrzE0a2P5oSUE8IdijHGBGSJIIR+2PkDGZszuGbANbj33zHGmAbHEkEITV4ymRhPDEcffrQNM22MabCsjSBEikuLmbJ8CkM6DeGSty+xYaaNMQ2WnRGEyJRlU9ixfwe5hbkUlhbatQPGmAbLzghCIK84j/FzxuMRD0u3LaWMMjzisWsHjDENkp0RhMB/vv0Pu/J2geIkATwM6zbMqoWMMQ1SSBOBiAwXkTUisk5ExldT7jgRKRWRUaGMpz7szt/Nw189zImdTiQ+Jh6veImPiWfi0ImWBIwxDVLIqoZExAs8DZwJbAYWiMh0VV0VoNyjwKehiqU+PfLVI+wt3Muk8yaxr3CfXUVsjGnwQtlGcDywTlXXA4jINOACYJVfuf8HvAccF8JY6kVxaTEvLXmJS/pcwjFtjgGwBGCMafBCWTXUEcjyeb7ZnVZBRDoCI4FJ1a1IRK4XkYUisnDnzp11HmhdmbNhDrvzdzO279hwh2KMMUELZSIIdCmt/80PngT+oqql1a1IVZ9X1TRVTTv88MPrKr4699bKt2ga25Rl25bZxWPGmIgRykSwGejk8zwF2OpXJg2YJiKZwCjgGRG5MIQxhUxhSSHvrHyHgpIC7vvyPs6YcoYlA2NMRAhlIlgA9BCRbiISB4wGpvsWUNVuqtpVVbsC7wI3q+qHIYwpZD796VP2F++3G88YYyJOyBqLVbVERG7F6Q3kBSar6koRudGdX227QKR5a+VbNItrRklZScVwEnbxmDEmEoT0ymJVnQnM9JsWMAGo6rhQxhJK+cX5TF8zndF9RnPNgGusy6gxJqLYEBN1YOKXE8ktyqVv27524xljTMSxISYO0cdrP+bvX/8dgPGzx1sDsTEm4lgiOER3/e+uisfWQGyMiURWNXQIZv04i+XblxPjiUFVrYHYGBORLBEcJFXl9k9vp1frXkw6dxLzs+ZbA7ExJiJZIjhIy7YvY032Gkb2GkmcN44JJ08Id0jGGHNQrI3gID2R8QQA09dMt6uIjTERzRLBQVBVPv7xYwC7itgYE/GsauggrNixguz8bGI9sZRpmTUSG2MimiWCg/DuqnfxiIcPLv2A5duXWyOxMSaiWSKopYysDCYtmkT/tv0596hzOfeoc8MdkjHGHBJrI6iFjKwMTp9yOjv272DFjhXWQGyMaRQsEdRCemY6hSWFAJRpmTUQG2MaBUsEtXBql1MrHlsDsTGmsbBEUAsx3hgU5aJeFzHnyjnWQGyMaRSssbgWpi6fSrw3npcueInmCc3DHY4xxtQJOyMIUklZCdNWTuO8o86zJGCMaVQsEQRp9vrZ7Ni/g7F9x4Y7FGOMqVOWCII09fupNE9ozjk9zgl3KMYYU6csEQQhtyiXd1e+y5EtjmTxz4vDHY4xxtSpkCYCERkuImtEZJ2IjA8w/wIRWS4iS0VkoYicFMp4DtZDcx+ioLSAxdsW20ijxphGJ2SJQES8wNPA2UBvYIyI9PYrNgfor6qpwDXAi6GK51BM/X4q4FxEZiONGmMam1CeERwPrFPV9apaBEwDLvAtoKq5qqru06aA0sD8sPMHNu3dRKwnFq947UIyY0yjE8rrCDoCWT7PNwOD/AuJyEjgYaANEHAENxG5HrgeoHPnznUeaHVeWvISMZ4YPhj9Acu32UijxpjGJ5SJQAJMO+CIX1U/AD4QkVOA+4FhAco8DzwPkJaWVm9nDUWlRby67FVG9BzBuT3O5dweNtKoMabxCWXV0Gagk8/zFGBrVYVVdS5whIi0DmFMtfLfNf9lV94ufjfgd+EOxRhjQiaUiWAB0ENEuolIHDAamO5bQESOFBFxHw8E4oDsEMYUNFXlnxn/pEtyF8464qxwh2OMMSETsqohVS0RkVuBTwEvMFlVV4rIje78ScDFwJUiUgzkA5f6NB6H1f82/I+MzRk8c84zeD3ecIdjjDEhE9JB51R1JjDTb9okn8ePAo+GMoaD9cC8B2jVpBW78naRkZVhDcTGmEbLriwO4KtNX5Gemc7ewr3c9+V9dhGZMaZRs0QQwP1z76dpbFPKtIxSLbWLyIwxjZolAj9fb/qaz376jGPaHEOMJ8YuIjPGNHqWCHyoKjfPvBmAhVsXoijXDbzO7kZmjGnU7A5lPmatm8Xy7csRhFIthTLonNzZkoBpVIqLi9m8eTMFBQXhDsWEQEJCAikpKcTGxga9jCUC19ebvuba6dfSOrE1+4v2U1RaZFVCplHavHkzzZo1o2vXrriX8ZhGQlXJzs5m8+bNdOvWLejlLBEAGVkZnD7l9Iqd/3/O/g/Zedk2rpBplAoKCiwJNFIiQqtWrdi5c2etlrNEAHyR+QVFpUUAlJaVkp2XzYSTJ4Q5KmNCx5JA43Uwn23UNxZnZGWwYMsCADx4rDrIGBN1ojoRZGRlcMaUM/hwzYcAjEsdZz2EjAmx7OxsUlNTSU1NpV27dnTs2LHieVFRUbXLLly4kN///vc1vsaQIUPqKtw6l5SUdMC0uXPnMnDgQGJiYnj33XfrPaaorhpKz0yvqBIShCNbHmlJwJgQa9WqFUuXLgVg4sSJJCUlceedd1bMLykpISYm8K4pLS2NtLS0Gl9j/vz5dRJrfencuTOvvPIKjz32WFheP6oTwdCuQ/GIh1ItJT4m3qqETNS57ZPbWLptaZ2uM7VdKk8Of7JWy4wbN46WLVuyZMkSBg4cyKWXXsptt91Gfn4+TZo04eWXX6Znz56kp6fz2GOPMWPGDCZOnMimTZtYv349mzZt4rbbbqs4W0hKSiI3N5f09HQmTpxI69atWbFiBcceeyyvv/46IsLMmTO54447aN26NQMHDmT9+vXMmDGjUlyZmZlcccUV7N+/H4Cnnnqq4mzj73//O6+99hoej4ezzz6bRx55hHXr1nHjjTeyc+dOvF4v77zzDkcccUSN29+1a1cAPJ7wVNJEdSIYlDKINk3bEOOJ4c2L37SzAWPCaO3atcyePRuv18vevXuZO3cuMTExzJ49m7vuuov33nvvgGVWr17NF198wb59++jZsyc33XTTAf3nlyxZwsqVK+nQoQMnnngiX3/9NWlpadxwww3MnTuXbt26MWbMmIAxtWnThs8//5yEhAR+/PFHxowZw8KFC5k1axYffvgh3377LYmJiezevRuAsWPHMn78eEaOHElBQQFlZWV1/0aFQFQngrkb57Jl3xZeH/m6JQETlWp75B5Kl1xyCV6vM+R7Tk4OV111FT/++CMiQnFxccBlzj33XOLj44mPj6dNmzZs376dlJSUSmWOP/74immpqalkZmaSlJRE9+7dK/rajxkzhueff/6A9RcXF3PrrbeydOlSvF4va9euBWD27NlcffXVJCYmAtCyZUv27dvHli1bGDlyJOBc2BUpojIRZGRlkJ6ZzsKtC2kS04QLe10Y7pCMiXpNmzateHz33Xdz2mmn8cEHH5CZmcnQoUMDLhMfH1/x2Ov1UlJSElSZYG978sQTT9C2bVuWLVtGWVlZxc5dVQ/optlAbqVyUKKu11B5T6G7v7ib91e/z6COg2ga17TmBY0x9SYnJ4eOHTsC8Morr9T5+nv16sX69evJzMwE4K233qoyjvbt2+PxeHjttdcoLS0F4KyzzmLy5Mnk5eUBsHv3bg477DBSUlL48MMPASgsLKyY39BFXSIo7ylUqs4H2japbZgjMsb4+/Of/8yECRM48cQTK3a+dalJkyY888wzDB8+nJNOOom2bduSnJx8QLmbb76ZV199lRNOOIG1a9dWnLUMHz6cESNGkJaWRmpqakVvn9dee41///vf9OvXjyFDhrBt27YD1pmXl0dKSkrF3+OPP86CBQtISUnhnXfe4YYbbqBPnz51vs3VkUg7nUlLS9OFCxce9PLlZwQFJQUoyudXfM6w7sPqMEJjGrYffviBo48+OtxhhF1ubi5JSUmoKrfccgs9evTg9ttvD3dYdSLQZywii1Q1YN/bqDsjGNxpMJ9f8TnN4ppxcueTLQkYE6VeeOEFUlNT6dOnDzk5Odxwww3hDilsorKx2CMe9hbt5fpjrw93KMaYMLn99tsbzRnAoQrpGYGIDBeRNSKyTkTGB5g/VkSWu3/zRaR/KOMp9+6qd4n1xHL+UefXx8sZY0yDFrJEICJe4GngbKA3MEZEevsV2wCcqqr9gPuBAzvyhsDHP37MGd3PIDnhwMYhY4yJNqE8IzgeWKeq61W1CJgGXOBbQFXnq+oe9+k3QAohtj13O2uy13Ba19NC/VLGGBMRQpkIOgJZPs83u9Oqci0wK9AMEbleRBaKyMLa3nDB37xN8wA4pcsph7QeY4xpLEKZCALdHSFgX1UROQ0nEfwl0HxVfV5V01Q17fDDDz+koOZtnEdibCID2w88pPUYYw7O0KFD+fTTTytNe/LJJ7n55purXaa82/g555zDL7/8ckCZiRMn1jh654cffsiqVasqnt9zzz3Mnj27FtHXn/ocrjqUiWAz0MnneQqw1b+QiPQDXgQuUNXsEMYDwNxNczkh5QTivHGhfiljGo2MrAwenvcwGVkZh7yuMWPGMG3atErTpk2bVuXAb/5mzpxJ8+bND+q1/RPB3/72N4YNi5wu5OXDVV922WV1ut5QJoIFQA8R6SYiccBoYLpvARHpDLwPXKGqa0MYCxlZGdzzxT0s3baUUzpbtZAxwfIdluWMKWcccjIYNWoUM2bMoLCwEHCGet66dSsnnXQSN910E2lpafTp04d777034PJdu3Zl165dADz44IP07NmTYcOGsWbNmooyL7zwAscddxz9+/fn4osvJi8vj/nz5zN9+nT+9Kc/kZqayk8//cS4ceMqjqznzJnDgAED6Nu3L9dcc01FfF27duXee+9l4MCB9O3bl9WrVx8QU2ZmJieffDIDBw5k4MCBle6H8Pe//52+ffvSv39/xo93Ok+uW7eOYcOG0b9/fwYOHMhPP/0U1HvXtWtX+vXrV+fDVYcsEahqCXAr8CnwA/C2qq4UkRtF5Ea32D1AK+AZEVkqIgd/yXA1yr/ID859EIBWia1C8TLGNEq+w7IUlRaRnpl+SOtr1aoVxx9/PJ988gngnA1ceumliAgPPvggCxcuZPny5Xz55ZcsX768yvUsWrSIadOmsWTJEt5//30WLFhQMe+iiy5iwYIFLFu2jKOPPpqXXnqJIUOGMGLECP7xj3+wdOnSSvcJKCgoYNy4cbz11lt8//33lJSU8Oyzz1bMb926NYsXL+amm24KWP1UPlz14sWLeeuttyrui+A7XPWyZcv485//DDjDVd9yyy0sW7aM+fPn0759+0N6Tw9VSK8jUNWZqnqUqh6hqg+60yap6iT38e9UtYWqprp/Nd966CCUf5HLcMYGz84PeQ2UMY3G0K5DifPG4RVvnd3T27d6yLda6O2332bgwIEMGDCAlStXVqrG8Tdv3jxGjhxJYmIihx12GCNGjKiYt2LFCk4++WT69u3L1KlTWblyZbXxrFmzhm7dunHUUUcBcNVVVzF37tyK+RdddBEAxx57bMVAdb6Ki4u57rrr6Nu3L5dccklF3MEOV10+P1yi4sri8i9yfkk+HvFwVvezwh2SMRFjcKfBzLlyDumZ6QztOrRO7t1x4YUXcscdd7B48WLy8/MZOHAgGzZs4LHHHmPBggW0aNGCcePGUVBQUO16/IeCLjdu3Dg+/PBD+vfvzyuvvEJ6enq166lpzLXyoayrGuo60oerjoqxhgZ3GszHl32MRzxcdsxldhMaY2ppcKfBTDh5Qp39dpKSkhg6dCjXXHNNxdnA3r17adq0KcnJyWzfvp1ZswL2Jq9wyimn8MEHH5Cfn8++ffv473//WzFv3759tG/fnuLiYqZOnVoxvVmzZuzbt++AdfXq1YvMzEzWrVsHOKOInnrqqUFvT6QPVx0ViQBAUcq0jDF9g+uZYIwJrTFjxrBs2TJGjx4NQP/+/RkwYAB9+vThmmuu4cQTT6x2+fJ7G6empnLxxRdz8sknV8y7//77GTRoEGeeeSa9evWqmD569Gj+8Y9/MGDAgEoNtAkJCbz88stccskl9O3bF4/Hw4033kiwIn246qgZhvqrTV/x8FcPM/WiqTRPaF73gRkTIWwY6savtsNQR0UbAcBJnU/i48s+DncYxhjT4ERN1ZAxxpjALBEYE4UirUrYBO9gPltLBMZEmYSEBLKzsy0ZNEKqSnZ2dkX31WBFTRuBMcaRkpLC5s2bOdSRfE3DlJCQQEpK7Ub0t0RgTJSJjY2lW7du4Q7DNCBWNWSMMVHOEoExxkQ5SwTGGBPlIu7KYhHZCWys5WKtgV0hCCccbFsaJtuWhqsxbc+hbEsXVQ14i8eISwQHQ0QWhmqI6/pm29Iw2bY0XI1pe0K1LVY1ZIwxUc4SgTHGRLloSQTPhzuAOmTb0jDZtjRcjWl7QrItUdFGYIwxpmrRckZgjDGmCpYIjDEmyjXqRCAiw0VkjYisE5Hx4Y6nNkSkk4h8ISI/iMhKEfmDO72liHwuIj+6/1uEO9ZgiYhXRJaIyAz3eSRvS3MReVdEVruf0eBI3R4Rud39jq0QkTdFJCFStkVEJovIDhFZ4TOtythFZIK7P1gjIr8JT9SBVbEt/3C/Y8tF5AMRae4zr862pdEmAhHxAk8DZwO9gTEi0ju8UdVKCfBHVT0aOAG4xY1/PDBHVXsAc9znkeIPwA8+zyN5W/4FfKKqvYD+ONsVcdsjIh2B3wNpqnoM4AVGEznb8gow3G9awNjd389ooI+7zDPufqKheIUDt+Vz4BhV7QesBSZA3W9Lo00EwPHAOlVdr6pFwDTggjDHFDRV/VlVF7uP9+HsaDribMOrbrFXgQvDEmAtiUgKcC7wos/kSN2Ww4BTgJcAVLVIVX8hQrcHZxTiJiISAyQCW4mQbVHVucBuv8lVxX4BME1VC1V1A7AOZz/RIATaFlX9TFVL3KffAOXjS9fptjTmRNARyPJ5vtmdFnFEpCswAPgWaKuqP4OTLIA2YQytNp4E/gyU+UyL1G3pDuwEXnarul4UkaZE4Pao6hbgMWAT8DOQo6qfEYHb4qOq2CN9n3ANMMt9XKfb0pgTgQSYFnF9ZUUkCXgPuE1V94Y7noMhIucBO1R1UbhjqSMxwEDgWVUdAOyn4VadVMutP78A6AZ0AJqKyOXhjSpkInafICJ/xakunlo+KUCxg96WxpwINgOdfJ6n4JzyRgwRicVJAlNV9X138nYRae/Obw/sCFd8tXAiMEJEMnGq6E4XkdeJzG0B57u1WVW/dZ+/i5MYInF7hgEbVHWnqhYD7wNDiMxtKVdV7BG5TxCRq4DzgLH664VfdbotjTkRLAB6iEg3EYnDaViZHuaYgiYiglMH/YOqPu4zazpwlfv4KuCj+o6ttlR1gqqmqGpXnM/hf6p6ORG4LQCqug3IEpGe7qQzgFVE5vZsAk4QkUT3O3cGTntUJG5Luapinw6MFpF4EekG9AC+C0N8QROR4cBfgBGqmuczq263RVUb7R9wDk5L+0/AX8MdTy1jPwnnVG85sNT9OwdohdMT4kf3f8twx1rL7RoKzHAfR+y2AKnAQvfz+RBoEanbA9wHrAZWAK8B8ZGyLcCbOG0bxThHyddWFzvwV3d/sAY4O9zxB7Et63DaAsr3AZNCsS02xIQxxkS5xlw1ZIwxJgiWCIwxJspZIjDGmChnicAYY6KcJQJjjIlylgiMcYlIqYgs9fmrs6uFRaSr76iSxjQkMeEOwJgGJF9VU8MdhDH1zc4IjKmBiGSKyKMi8p37d6Q7vYuIzHHHip8jIp3d6W3dseOXuX9D3FV5ReQFd+z/z0SkiVv+9yKyyl3PtDBtpolilgiM+VUTv6qhS33m7VXV44GncEZSxX08RZ2x4qcC/3an/xv4UlX744xBtNKd3gN4WlX7AL8AF7vTxwMD3PXcGJpNM6ZqdmWxMS4RyVXVpADTM4HTVXW9OxDgNlVtJSK7gPaqWuxO/1lVW4vITiBFVQt91tEV+Fydm6UgIn8BYlX1ARH5BMjFGariQ1XNDfGmGlOJnREYExyt4nFVZQIp9Hlcyq9tdOfi3E3vWGCRe4MYY+qNJQJjgnOpz/8M9/F8nNFUAcYCX7mP5wA3QcV9mg+raqUi4gE6qeoXODfuaQ4ccFZiTCjZkYcxv2oiIkt9nn+iquVdSONF5Fucg6cx7rTfA5NF5E84dyy72p3+B+B5EbkW58j/JpxRJQPxAq+LSDLOzUaeUOe2l8bUG2sjMKYGbhtBmqruCncsxoSCVQ0ZY0yUszMCY4yJcnZGYIwxUc4SgTHGRDlLBMYYE+UsERhjTJSzRGCMMVHu/wNHfvj/CAPpFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['accuracy'] \n",
    "val_acc_values = L1_model_dict['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer.\n",
    "\n",
    "To complete our comparison, let's use `model.evaluate()` again on the appropriate variables to compare results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 2ms/step - loss: 1.1244 - accuracy: 0.8556\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.2924 - accuracy: 0.7653\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1243808269500732, 0.8555999994277954]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output: [1.3186310468037923, 0.72266666663487755]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2923870086669922, 0.765333354473114]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [1.3541648308436076, 0.70800000031789145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dropout Regularization\n",
    "\n",
    "Dropout Regularization is accomplished by adding in an additional `Dropout` layer wherever we want to use it, and providing a percentage value for how likely any given neuron is to get \"dropped out\" during this layer. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Dropout` from `keras.layers`\n",
    "* Recreate the same network we have above, but this time without any L1 or L2 regularization\n",
    "* Add a `Dropout` layer between hidden layer 1 and hidden layer 2.  This should have a dropout chance of `0.3`.\n",
    "* Add a `Dropout` layer between hidden layer 2 and the output layer.  This should have a dropout chance of `0.3`.\n",
    "* Compile the model with the exact same hyperparameters as all other models we've built. \n",
    "* Fit the model with the same hyperparameters we've used above.  But this time, train the model for `200` epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 1s 16ms/step - loss: 1.9630 - accuracy: 0.1449 - val_loss: 1.9366 - val_accuracy: 0.1530\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.9417 - accuracy: 0.1633 - val_loss: 1.9218 - val_accuracy: 0.1730\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.9277 - accuracy: 0.1788 - val_loss: 1.9095 - val_accuracy: 0.2050\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9167 - accuracy: 0.1996 - val_loss: 1.8974 - val_accuracy: 0.2280\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9036 - accuracy: 0.2069 - val_loss: 1.8842 - val_accuracy: 0.2440\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.8919 - accuracy: 0.2128 - val_loss: 1.8689 - val_accuracy: 0.2560\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.8777 - accuracy: 0.2228 - val_loss: 1.8522 - val_accuracy: 0.2740\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.8597 - accuracy: 0.2333 - val_loss: 1.8314 - val_accuracy: 0.2860\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8451 - accuracy: 0.2528 - val_loss: 1.8110 - val_accuracy: 0.3020\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.8316 - accuracy: 0.2591 - val_loss: 1.7909 - val_accuracy: 0.3230\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.8133 - accuracy: 0.2745 - val_loss: 1.7677 - val_accuracy: 0.3300\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.7925 - accuracy: 0.2871 - val_loss: 1.7447 - val_accuracy: 0.3520\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.7769 - accuracy: 0.2945 - val_loss: 1.7203 - val_accuracy: 0.3540\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.7546 - accuracy: 0.3067 - val_loss: 1.6956 - val_accuracy: 0.3770\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.7306 - accuracy: 0.3191 - val_loss: 1.6696 - val_accuracy: 0.3940\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.7021 - accuracy: 0.3368 - val_loss: 1.6425 - val_accuracy: 0.4160\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6933 - accuracy: 0.3391 - val_loss: 1.6172 - val_accuracy: 0.4320\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.6652 - accuracy: 0.3544 - val_loss: 1.5904 - val_accuracy: 0.4540\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.6490 - accuracy: 0.3593 - val_loss: 1.5615 - val_accuracy: 0.4670\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.6193 - accuracy: 0.3773 - val_loss: 1.5329 - val_accuracy: 0.4730\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.5964 - accuracy: 0.3861 - val_loss: 1.5056 - val_accuracy: 0.5030\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.5774 - accuracy: 0.3931 - val_loss: 1.4796 - val_accuracy: 0.5140\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.5588 - accuracy: 0.4045 - val_loss: 1.4527 - val_accuracy: 0.5260\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.5388 - accuracy: 0.4179 - val_loss: 1.4280 - val_accuracy: 0.5420\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.5096 - accuracy: 0.4228 - val_loss: 1.4019 - val_accuracy: 0.5500\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.4922 - accuracy: 0.4313 - val_loss: 1.3752 - val_accuracy: 0.5620\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.4699 - accuracy: 0.4483 - val_loss: 1.3506 - val_accuracy: 0.5740\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.4498 - accuracy: 0.4605 - val_loss: 1.3262 - val_accuracy: 0.5760\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.4312 - accuracy: 0.4651 - val_loss: 1.3031 - val_accuracy: 0.5850\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.4065 - accuracy: 0.4764 - val_loss: 1.2810 - val_accuracy: 0.5910\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.3880 - accuracy: 0.4777 - val_loss: 1.2575 - val_accuracy: 0.6000\n",
      "Epoch 32/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3682 - accuracy: 0.4879 - val_loss: 1.2343 - val_accuracy: 0.6070\n",
      "Epoch 33/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3623 - accuracy: 0.4844 - val_loss: 1.2161 - val_accuracy: 0.6130\n",
      "Epoch 34/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3461 - accuracy: 0.4964 - val_loss: 1.1960 - val_accuracy: 0.6210\n",
      "Epoch 35/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3221 - accuracy: 0.5076 - val_loss: 1.1771 - val_accuracy: 0.6320\n",
      "Epoch 36/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2956 - accuracy: 0.5213 - val_loss: 1.1548 - val_accuracy: 0.6340\n",
      "Epoch 37/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2967 - accuracy: 0.5067 - val_loss: 1.1409 - val_accuracy: 0.6360\n",
      "Epoch 38/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2758 - accuracy: 0.5189 - val_loss: 1.1237 - val_accuracy: 0.6430\n",
      "Epoch 39/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2550 - accuracy: 0.5287 - val_loss: 1.1061 - val_accuracy: 0.6460\n",
      "Epoch 40/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2400 - accuracy: 0.5379 - val_loss: 1.0905 - val_accuracy: 0.6550\n",
      "Epoch 41/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.2284 - accuracy: 0.5399 - val_loss: 1.0759 - val_accuracy: 0.6600\n",
      "Epoch 42/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2094 - accuracy: 0.5500 - val_loss: 1.0611 - val_accuracy: 0.6640\n",
      "Epoch 43/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2017 - accuracy: 0.5563 - val_loss: 1.0471 - val_accuracy: 0.6630\n",
      "Epoch 44/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1781 - accuracy: 0.5571 - val_loss: 1.0342 - val_accuracy: 0.6680\n",
      "Epoch 45/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1777 - accuracy: 0.5640 - val_loss: 1.0225 - val_accuracy: 0.6690\n",
      "Epoch 46/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1671 - accuracy: 0.5537 - val_loss: 1.0095 - val_accuracy: 0.6720\n",
      "Epoch 47/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1535 - accuracy: 0.5664 - val_loss: 0.9959 - val_accuracy: 0.6840\n",
      "Epoch 48/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1432 - accuracy: 0.5657 - val_loss: 0.9860 - val_accuracy: 0.6780\n",
      "Epoch 49/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.1276 - accuracy: 0.5723 - val_loss: 0.9764 - val_accuracy: 0.6820\n",
      "Epoch 50/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1205 - accuracy: 0.5728 - val_loss: 0.9665 - val_accuracy: 0.6810\n",
      "Epoch 51/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.1143 - accuracy: 0.5840 - val_loss: 0.9562 - val_accuracy: 0.6840\n",
      "Epoch 52/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0935 - accuracy: 0.5893 - val_loss: 0.9434 - val_accuracy: 0.6910\n",
      "Epoch 53/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0885 - accuracy: 0.5877 - val_loss: 0.9384 - val_accuracy: 0.6890\n",
      "Epoch 54/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0786 - accuracy: 0.5924 - val_loss: 0.9254 - val_accuracy: 0.6970\n",
      "Epoch 55/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0802 - accuracy: 0.5896 - val_loss: 0.9177 - val_accuracy: 0.6960\n",
      "Epoch 56/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0665 - accuracy: 0.6004 - val_loss: 0.9119 - val_accuracy: 0.6960\n",
      "Epoch 57/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0604 - accuracy: 0.6005 - val_loss: 0.9058 - val_accuracy: 0.7000\n",
      "Epoch 58/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.0425 - accuracy: 0.5977 - val_loss: 0.8945 - val_accuracy: 0.7050\n",
      "Epoch 59/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0379 - accuracy: 0.6128 - val_loss: 0.8852 - val_accuracy: 0.7040\n",
      "Epoch 60/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.0351 - accuracy: 0.6136 - val_loss: 0.8792 - val_accuracy: 0.7100\n",
      "Epoch 61/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0175 - accuracy: 0.6201 - val_loss: 0.8720 - val_accuracy: 0.7060\n",
      "Epoch 62/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.0054 - accuracy: 0.6199 - val_loss: 0.8634 - val_accuracy: 0.7110\n",
      "Epoch 63/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.9997 - accuracy: 0.6261 - val_loss: 0.8577 - val_accuracy: 0.7080\n",
      "Epoch 64/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0001 - accuracy: 0.6208 - val_loss: 0.8521 - val_accuracy: 0.7170\n",
      "Epoch 65/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.9878 - accuracy: 0.6287 - val_loss: 0.8461 - val_accuracy: 0.7200\n",
      "Epoch 66/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.9880 - accuracy: 0.6240 - val_loss: 0.8402 - val_accuracy: 0.7140\n",
      "Epoch 67/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.9745 - accuracy: 0.6312 - val_loss: 0.8333 - val_accuracy: 0.7130\n",
      "Epoch 68/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9673 - accuracy: 0.6371 - val_loss: 0.8276 - val_accuracy: 0.7190\n",
      "Epoch 69/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.9518 - accuracy: 0.6420 - val_loss: 0.8224 - val_accuracy: 0.7250\n",
      "Epoch 70/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9553 - accuracy: 0.6416 - val_loss: 0.8169 - val_accuracy: 0.7190\n",
      "Epoch 71/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9455 - accuracy: 0.6445 - val_loss: 0.8117 - val_accuracy: 0.7250\n",
      "Epoch 72/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9307 - accuracy: 0.6504 - val_loss: 0.8059 - val_accuracy: 0.7230\n",
      "Epoch 73/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9434 - accuracy: 0.6519 - val_loss: 0.8008 - val_accuracy: 0.7260\n",
      "Epoch 74/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.9155 - accuracy: 0.6557 - val_loss: 0.7946 - val_accuracy: 0.7280\n",
      "Epoch 75/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9225 - accuracy: 0.6491 - val_loss: 0.7910 - val_accuracy: 0.7270\n",
      "Epoch 76/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9236 - accuracy: 0.6519 - val_loss: 0.7875 - val_accuracy: 0.7270\n",
      "Epoch 77/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9119 - accuracy: 0.6561 - val_loss: 0.7818 - val_accuracy: 0.7300\n",
      "Epoch 78/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.9048 - accuracy: 0.6560 - val_loss: 0.7789 - val_accuracy: 0.7300\n",
      "Epoch 79/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8928 - accuracy: 0.6648 - val_loss: 0.7730 - val_accuracy: 0.7330\n",
      "Epoch 80/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8930 - accuracy: 0.6653 - val_loss: 0.7707 - val_accuracy: 0.7310\n",
      "Epoch 81/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8869 - accuracy: 0.6696 - val_loss: 0.7650 - val_accuracy: 0.7330\n",
      "Epoch 82/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8738 - accuracy: 0.6655 - val_loss: 0.7609 - val_accuracy: 0.7330\n",
      "Epoch 83/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8672 - accuracy: 0.6768 - val_loss: 0.7563 - val_accuracy: 0.7320\n",
      "Epoch 84/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8701 - accuracy: 0.6765 - val_loss: 0.7529 - val_accuracy: 0.7340\n",
      "Epoch 85/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8733 - accuracy: 0.6684 - val_loss: 0.7523 - val_accuracy: 0.7350\n",
      "Epoch 86/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8651 - accuracy: 0.6803 - val_loss: 0.7486 - val_accuracy: 0.7380\n",
      "Epoch 87/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8531 - accuracy: 0.6809 - val_loss: 0.7456 - val_accuracy: 0.7380\n",
      "Epoch 88/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8480 - accuracy: 0.6843 - val_loss: 0.7434 - val_accuracy: 0.7380\n",
      "Epoch 89/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8449 - accuracy: 0.6851 - val_loss: 0.7360 - val_accuracy: 0.7350\n",
      "Epoch 90/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8390 - accuracy: 0.6817 - val_loss: 0.7353 - val_accuracy: 0.7400\n",
      "Epoch 91/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8428 - accuracy: 0.6795 - val_loss: 0.7316 - val_accuracy: 0.7430\n",
      "Epoch 92/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8337 - accuracy: 0.6832 - val_loss: 0.7295 - val_accuracy: 0.7420\n",
      "Epoch 93/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8392 - accuracy: 0.6861 - val_loss: 0.7269 - val_accuracy: 0.7410\n",
      "Epoch 94/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8235 - accuracy: 0.6969 - val_loss: 0.7247 - val_accuracy: 0.7400\n",
      "Epoch 95/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8185 - accuracy: 0.6967 - val_loss: 0.7229 - val_accuracy: 0.7410\n",
      "Epoch 96/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8110 - accuracy: 0.6932 - val_loss: 0.7188 - val_accuracy: 0.7420\n",
      "Epoch 97/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7985 - accuracy: 0.7017 - val_loss: 0.7191 - val_accuracy: 0.7410\n",
      "Epoch 98/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8128 - accuracy: 0.6947 - val_loss: 0.7137 - val_accuracy: 0.7460\n",
      "Epoch 99/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8043 - accuracy: 0.7008 - val_loss: 0.7118 - val_accuracy: 0.7470\n",
      "Epoch 100/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7956 - accuracy: 0.7033 - val_loss: 0.7084 - val_accuracy: 0.7470\n",
      "Epoch 101/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7934 - accuracy: 0.7071 - val_loss: 0.7067 - val_accuracy: 0.7460\n",
      "Epoch 102/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7886 - accuracy: 0.7055 - val_loss: 0.7046 - val_accuracy: 0.7450\n",
      "Epoch 103/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8003 - accuracy: 0.7025 - val_loss: 0.7040 - val_accuracy: 0.7500\n",
      "Epoch 104/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7769 - accuracy: 0.7039 - val_loss: 0.7002 - val_accuracy: 0.7450\n",
      "Epoch 105/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7750 - accuracy: 0.7055 - val_loss: 0.6966 - val_accuracy: 0.7480\n",
      "Epoch 106/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7741 - accuracy: 0.7108 - val_loss: 0.6953 - val_accuracy: 0.7450\n",
      "Epoch 107/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7643 - accuracy: 0.7133 - val_loss: 0.6948 - val_accuracy: 0.7440\n",
      "Epoch 108/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7661 - accuracy: 0.7101 - val_loss: 0.6949 - val_accuracy: 0.7470\n",
      "Epoch 109/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7644 - accuracy: 0.7109 - val_loss: 0.6909 - val_accuracy: 0.7480\n",
      "Epoch 110/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7522 - accuracy: 0.7199 - val_loss: 0.6886 - val_accuracy: 0.7460\n",
      "Epoch 111/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7569 - accuracy: 0.7188 - val_loss: 0.6865 - val_accuracy: 0.7490\n",
      "Epoch 112/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7551 - accuracy: 0.7187 - val_loss: 0.6860 - val_accuracy: 0.7500\n",
      "Epoch 113/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7430 - accuracy: 0.7197 - val_loss: 0.6841 - val_accuracy: 0.7480\n",
      "Epoch 114/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7394 - accuracy: 0.7211 - val_loss: 0.6842 - val_accuracy: 0.7480\n",
      "Epoch 115/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7381 - accuracy: 0.7223 - val_loss: 0.6813 - val_accuracy: 0.7470\n",
      "Epoch 116/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7356 - accuracy: 0.7308 - val_loss: 0.6796 - val_accuracy: 0.7450\n",
      "Epoch 117/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7325 - accuracy: 0.7260 - val_loss: 0.6779 - val_accuracy: 0.7480\n",
      "Epoch 118/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.7346 - accuracy: 0.7213 - val_loss: 0.6741 - val_accuracy: 0.7490\n",
      "Epoch 119/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7296 - accuracy: 0.7272 - val_loss: 0.6744 - val_accuracy: 0.7470\n",
      "Epoch 120/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7214 - accuracy: 0.7300 - val_loss: 0.6740 - val_accuracy: 0.7480\n",
      "Epoch 121/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7183 - accuracy: 0.7299 - val_loss: 0.6705 - val_accuracy: 0.7470\n",
      "Epoch 122/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7165 - accuracy: 0.7364 - val_loss: 0.6696 - val_accuracy: 0.7500\n",
      "Epoch 123/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7165 - accuracy: 0.7303 - val_loss: 0.6674 - val_accuracy: 0.7490\n",
      "Epoch 124/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7143 - accuracy: 0.7268 - val_loss: 0.6649 - val_accuracy: 0.7470\n",
      "Epoch 125/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7086 - accuracy: 0.7280 - val_loss: 0.6640 - val_accuracy: 0.7470\n",
      "Epoch 126/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6995 - accuracy: 0.7397 - val_loss: 0.6641 - val_accuracy: 0.7500\n",
      "Epoch 127/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6861 - accuracy: 0.7437 - val_loss: 0.6632 - val_accuracy: 0.7490\n",
      "Epoch 128/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.7013 - accuracy: 0.7392 - val_loss: 0.6604 - val_accuracy: 0.7460\n",
      "Epoch 129/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6876 - accuracy: 0.7392 - val_loss: 0.6576 - val_accuracy: 0.7490\n",
      "Epoch 130/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6874 - accuracy: 0.7413 - val_loss: 0.6580 - val_accuracy: 0.7490\n",
      "Epoch 131/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6905 - accuracy: 0.7444 - val_loss: 0.6571 - val_accuracy: 0.7510\n",
      "Epoch 132/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6816 - accuracy: 0.7416 - val_loss: 0.6583 - val_accuracy: 0.7490\n",
      "Epoch 133/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6731 - accuracy: 0.7429 - val_loss: 0.6582 - val_accuracy: 0.7480\n",
      "Epoch 134/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6876 - accuracy: 0.7387 - val_loss: 0.6562 - val_accuracy: 0.7520\n",
      "Epoch 135/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6891 - accuracy: 0.7397 - val_loss: 0.6553 - val_accuracy: 0.7500\n",
      "Epoch 136/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6676 - accuracy: 0.7519 - val_loss: 0.6555 - val_accuracy: 0.7560\n",
      "Epoch 137/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6728 - accuracy: 0.7484 - val_loss: 0.6513 - val_accuracy: 0.7530\n",
      "Epoch 138/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6704 - accuracy: 0.7492 - val_loss: 0.6525 - val_accuracy: 0.7560\n",
      "Epoch 139/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6665 - accuracy: 0.7449 - val_loss: 0.6483 - val_accuracy: 0.7550\n",
      "Epoch 140/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6614 - accuracy: 0.7509 - val_loss: 0.6490 - val_accuracy: 0.7500\n",
      "Epoch 141/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6619 - accuracy: 0.7480 - val_loss: 0.6463 - val_accuracy: 0.7490\n",
      "Epoch 142/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6541 - accuracy: 0.7515 - val_loss: 0.6501 - val_accuracy: 0.7560\n",
      "Epoch 143/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6465 - accuracy: 0.7641 - val_loss: 0.6472 - val_accuracy: 0.7580\n",
      "Epoch 144/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6453 - accuracy: 0.7503 - val_loss: 0.6453 - val_accuracy: 0.7570\n",
      "Epoch 145/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6480 - accuracy: 0.7568 - val_loss: 0.6455 - val_accuracy: 0.7600\n",
      "Epoch 146/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6484 - accuracy: 0.7556 - val_loss: 0.6453 - val_accuracy: 0.7570\n",
      "Epoch 147/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6514 - accuracy: 0.7592 - val_loss: 0.6470 - val_accuracy: 0.7580\n",
      "Epoch 148/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6390 - accuracy: 0.7563 - val_loss: 0.6428 - val_accuracy: 0.7570\n",
      "Epoch 149/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6270 - accuracy: 0.7664 - val_loss: 0.6415 - val_accuracy: 0.7590\n",
      "Epoch 150/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6384 - accuracy: 0.7589 - val_loss: 0.6419 - val_accuracy: 0.7590\n",
      "Epoch 151/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6305 - accuracy: 0.7635 - val_loss: 0.6397 - val_accuracy: 0.7590\n",
      "Epoch 152/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6288 - accuracy: 0.7639 - val_loss: 0.6415 - val_accuracy: 0.7610\n",
      "Epoch 153/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6292 - accuracy: 0.7680 - val_loss: 0.6397 - val_accuracy: 0.7620\n",
      "Epoch 154/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6171 - accuracy: 0.7677 - val_loss: 0.6378 - val_accuracy: 0.7600\n",
      "Epoch 155/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6171 - accuracy: 0.7681 - val_loss: 0.6404 - val_accuracy: 0.7600\n",
      "Epoch 156/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6203 - accuracy: 0.7672 - val_loss: 0.6403 - val_accuracy: 0.7620\n",
      "Epoch 157/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6241 - accuracy: 0.7665 - val_loss: 0.6381 - val_accuracy: 0.7610\n",
      "Epoch 158/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6164 - accuracy: 0.7672 - val_loss: 0.6371 - val_accuracy: 0.7580\n",
      "Epoch 159/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6125 - accuracy: 0.7716 - val_loss: 0.6365 - val_accuracy: 0.7600\n",
      "Epoch 160/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6039 - accuracy: 0.7727 - val_loss: 0.6357 - val_accuracy: 0.7600\n",
      "Epoch 161/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6014 - accuracy: 0.7747 - val_loss: 0.6348 - val_accuracy: 0.7610\n",
      "Epoch 162/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6045 - accuracy: 0.7769 - val_loss: 0.6344 - val_accuracy: 0.7610\n",
      "Epoch 163/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5974 - accuracy: 0.7749 - val_loss: 0.6370 - val_accuracy: 0.7600\n",
      "Epoch 164/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5972 - accuracy: 0.7761 - val_loss: 0.6348 - val_accuracy: 0.7590\n",
      "Epoch 165/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5999 - accuracy: 0.7765 - val_loss: 0.6331 - val_accuracy: 0.7630\n",
      "Epoch 166/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6025 - accuracy: 0.7769 - val_loss: 0.6352 - val_accuracy: 0.7600\n",
      "Epoch 167/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5950 - accuracy: 0.7741 - val_loss: 0.6341 - val_accuracy: 0.7610\n",
      "Epoch 168/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5968 - accuracy: 0.7695 - val_loss: 0.6333 - val_accuracy: 0.7600\n",
      "Epoch 169/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5916 - accuracy: 0.7805 - val_loss: 0.6319 - val_accuracy: 0.7610\n",
      "Epoch 170/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5831 - accuracy: 0.7777 - val_loss: 0.6322 - val_accuracy: 0.7630\n",
      "Epoch 171/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5954 - accuracy: 0.7760 - val_loss: 0.6308 - val_accuracy: 0.7610\n",
      "Epoch 172/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5769 - accuracy: 0.7848 - val_loss: 0.6305 - val_accuracy: 0.7590\n",
      "Epoch 173/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5923 - accuracy: 0.7708 - val_loss: 0.6312 - val_accuracy: 0.7610\n",
      "Epoch 174/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5828 - accuracy: 0.7852 - val_loss: 0.6287 - val_accuracy: 0.7620\n",
      "Epoch 175/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5788 - accuracy: 0.7815 - val_loss: 0.6299 - val_accuracy: 0.7620\n",
      "Epoch 176/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5767 - accuracy: 0.7893 - val_loss: 0.6288 - val_accuracy: 0.7610\n",
      "Epoch 177/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5733 - accuracy: 0.7807 - val_loss: 0.6274 - val_accuracy: 0.7610\n",
      "Epoch 178/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5736 - accuracy: 0.7861 - val_loss: 0.6293 - val_accuracy: 0.7620\n",
      "Epoch 179/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5665 - accuracy: 0.7895 - val_loss: 0.6290 - val_accuracy: 0.7630\n",
      "Epoch 180/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5760 - accuracy: 0.7821 - val_loss: 0.6264 - val_accuracy: 0.7600\n",
      "Epoch 181/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5804 - accuracy: 0.7829 - val_loss: 0.6250 - val_accuracy: 0.7610\n",
      "Epoch 182/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5674 - accuracy: 0.7849 - val_loss: 0.6282 - val_accuracy: 0.7610\n",
      "Epoch 183/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5574 - accuracy: 0.7897 - val_loss: 0.6279 - val_accuracy: 0.7630\n",
      "Epoch 184/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5604 - accuracy: 0.7860 - val_loss: 0.6272 - val_accuracy: 0.7630\n",
      "Epoch 185/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5577 - accuracy: 0.7976 - val_loss: 0.6270 - val_accuracy: 0.7610\n",
      "Epoch 186/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5537 - accuracy: 0.7859 - val_loss: 0.6269 - val_accuracy: 0.7620\n",
      "Epoch 187/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5578 - accuracy: 0.7937 - val_loss: 0.6269 - val_accuracy: 0.7660\n",
      "Epoch 188/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5544 - accuracy: 0.7937 - val_loss: 0.6268 - val_accuracy: 0.7640\n",
      "Epoch 189/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5501 - accuracy: 0.7901 - val_loss: 0.6237 - val_accuracy: 0.7660\n",
      "Epoch 190/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5457 - accuracy: 0.7960 - val_loss: 0.6246 - val_accuracy: 0.7640\n",
      "Epoch 191/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5537 - accuracy: 0.7919 - val_loss: 0.6252 - val_accuracy: 0.7620\n",
      "Epoch 192/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5454 - accuracy: 0.7988 - val_loss: 0.6244 - val_accuracy: 0.7610\n",
      "Epoch 193/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5432 - accuracy: 0.7908 - val_loss: 0.6231 - val_accuracy: 0.7610\n",
      "Epoch 194/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5414 - accuracy: 0.7963 - val_loss: 0.6228 - val_accuracy: 0.7690\n",
      "Epoch 195/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5457 - accuracy: 0.7929 - val_loss: 0.6215 - val_accuracy: 0.7700\n",
      "Epoch 196/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5389 - accuracy: 0.8036 - val_loss: 0.6245 - val_accuracy: 0.7660\n",
      "Epoch 197/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5358 - accuracy: 0.8044 - val_loss: 0.6227 - val_accuracy: 0.7690\n",
      "Epoch 198/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5283 - accuracy: 0.8051 - val_loss: 0.6212 - val_accuracy: 0.7740\n",
      "Epoch 199/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5327 - accuracy: 0.7981 - val_loss: 0.6236 - val_accuracy: 0.7680\n",
      "Epoch 200/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5353 - accuracy: 0.7963 - val_loss: 0.6221 - val_accuracy: 0.7670\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='SGD',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "dropout_model = model.fit(train_final, label_train_final, epochs=200, batch_size=256,validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the results from `model.evaluate` to see how this change has affected our training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 1ms/step - loss: 0.3704 - accuracy: 0.8681\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.6097 - accuracy: 0.7693\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3704076409339905, 0.8681333065032959]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.36925017188787462, 0.88026666666666664]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6097486019134521, 0.7693333625793457]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.69210424280166627, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! However, the variance did become higher again, compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  More Training Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to high variance is to just get more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets.\n",
    "\n",
    "Run the cell below to preprocess our entire dataset, instead of just working with a subset of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the first model that we built, without any regularization or dropout layers included. \n",
    "\n",
    "Train this model for 120 epochs.  All other hyperparameters should stay the same.  Store the fitted model inside of `moredata_model`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "129/129 [==============================] - 2s 8ms/step - loss: 1.9406 - accuracy: 0.1848 - val_loss: 1.9161 - val_accuracy: 0.2413\n",
      "Epoch 2/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 1.8746 - accuracy: 0.2858 - val_loss: 1.8283 - val_accuracy: 0.3327\n",
      "Epoch 3/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 1.7491 - accuracy: 0.3683 - val_loss: 1.6662 - val_accuracy: 0.4143\n",
      "Epoch 4/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 1.5616 - accuracy: 0.4625 - val_loss: 1.4534 - val_accuracy: 0.5160\n",
      "Epoch 5/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 1.3508 - accuracy: 0.5652 - val_loss: 1.2487 - val_accuracy: 0.6160\n",
      "Epoch 6/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 1.1690 - accuracy: 0.6307 - val_loss: 1.0882 - val_accuracy: 0.6583\n",
      "Epoch 7/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 1.0322 - accuracy: 0.6665 - val_loss: 0.9738 - val_accuracy: 0.6840\n",
      "Epoch 8/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.9333 - accuracy: 0.6886 - val_loss: 0.8912 - val_accuracy: 0.7060\n",
      "Epoch 9/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.8614 - accuracy: 0.7046 - val_loss: 0.8316 - val_accuracy: 0.7213\n",
      "Epoch 10/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.8078 - accuracy: 0.7167 - val_loss: 0.7871 - val_accuracy: 0.7303\n",
      "Epoch 11/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.7657 - accuracy: 0.7271 - val_loss: 0.7533 - val_accuracy: 0.7380\n",
      "Epoch 12/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.7329 - accuracy: 0.7377 - val_loss: 0.7263 - val_accuracy: 0.7443\n",
      "Epoch 13/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.7058 - accuracy: 0.7454 - val_loss: 0.7062 - val_accuracy: 0.7480\n",
      "Epoch 14/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.6831 - accuracy: 0.7514 - val_loss: 0.6868 - val_accuracy: 0.7563\n",
      "Epoch 15/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.6633 - accuracy: 0.7592 - val_loss: 0.6741 - val_accuracy: 0.7547\n",
      "Epoch 16/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.6464 - accuracy: 0.7629 - val_loss: 0.6588 - val_accuracy: 0.7600\n",
      "Epoch 17/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.6312 - accuracy: 0.7684 - val_loss: 0.6487 - val_accuracy: 0.7623\n",
      "Epoch 18/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.6179 - accuracy: 0.7728 - val_loss: 0.6386 - val_accuracy: 0.7587\n",
      "Epoch 19/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.6060 - accuracy: 0.7768 - val_loss: 0.6304 - val_accuracy: 0.7667\n",
      "Epoch 20/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.5948 - accuracy: 0.7826 - val_loss: 0.6227 - val_accuracy: 0.7707\n",
      "Epoch 21/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.5846 - accuracy: 0.7858 - val_loss: 0.6166 - val_accuracy: 0.7720\n",
      "Epoch 22/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.5757 - accuracy: 0.7900 - val_loss: 0.6121 - val_accuracy: 0.7747\n",
      "Epoch 23/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.5671 - accuracy: 0.7930 - val_loss: 0.6045 - val_accuracy: 0.7773\n",
      "Epoch 24/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.5590 - accuracy: 0.7948 - val_loss: 0.6005 - val_accuracy: 0.7777\n",
      "Epoch 25/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.5518 - accuracy: 0.7982 - val_loss: 0.5959 - val_accuracy: 0.7787\n",
      "Epoch 26/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.5447 - accuracy: 0.8025 - val_loss: 0.5905 - val_accuracy: 0.7823\n",
      "Epoch 27/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.5380 - accuracy: 0.8042 - val_loss: 0.5888 - val_accuracy: 0.7827\n",
      "Epoch 28/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.5321 - accuracy: 0.8066 - val_loss: 0.5862 - val_accuracy: 0.7850\n",
      "Epoch 29/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.5260 - accuracy: 0.8098 - val_loss: 0.5820 - val_accuracy: 0.7860\n",
      "Epoch 30/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.5202 - accuracy: 0.8110 - val_loss: 0.5806 - val_accuracy: 0.7857\n",
      "Epoch 31/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.5151 - accuracy: 0.8137 - val_loss: 0.5755 - val_accuracy: 0.7910\n",
      "Epoch 32/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.5101 - accuracy: 0.8159 - val_loss: 0.5736 - val_accuracy: 0.7897\n",
      "Epoch 33/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.5048 - accuracy: 0.8166 - val_loss: 0.5703 - val_accuracy: 0.7913\n",
      "Epoch 34/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.4997 - accuracy: 0.8205 - val_loss: 0.5682 - val_accuracy: 0.7913\n",
      "Epoch 35/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.4951 - accuracy: 0.8225 - val_loss: 0.5672 - val_accuracy: 0.7933\n",
      "Epoch 36/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.4910 - accuracy: 0.8227 - val_loss: 0.5661 - val_accuracy: 0.7937\n",
      "Epoch 37/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.4862 - accuracy: 0.8251 - val_loss: 0.5628 - val_accuracy: 0.7943\n",
      "Epoch 38/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.4823 - accuracy: 0.8265 - val_loss: 0.5612 - val_accuracy: 0.7947\n",
      "Epoch 39/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.4783 - accuracy: 0.8286 - val_loss: 0.5627 - val_accuracy: 0.7900\n",
      "Epoch 40/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4741 - accuracy: 0.8305 - val_loss: 0.5589 - val_accuracy: 0.7947\n",
      "Epoch 41/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.4704 - accuracy: 0.8312 - val_loss: 0.5584 - val_accuracy: 0.7933\n",
      "Epoch 42/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4666 - accuracy: 0.8328 - val_loss: 0.5577 - val_accuracy: 0.7933\n",
      "Epoch 43/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.4632 - accuracy: 0.8338 - val_loss: 0.5573 - val_accuracy: 0.7923\n",
      "Epoch 44/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4596 - accuracy: 0.8362 - val_loss: 0.5550 - val_accuracy: 0.7950\n",
      "Epoch 45/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4558 - accuracy: 0.8375 - val_loss: 0.5566 - val_accuracy: 0.7913\n",
      "Epoch 46/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4528 - accuracy: 0.8381 - val_loss: 0.5544 - val_accuracy: 0.7947\n",
      "Epoch 47/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4493 - accuracy: 0.8399 - val_loss: 0.5523 - val_accuracy: 0.7947\n",
      "Epoch 48/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4463 - accuracy: 0.8401 - val_loss: 0.5511 - val_accuracy: 0.7953\n",
      "Epoch 49/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4430 - accuracy: 0.8424 - val_loss: 0.5504 - val_accuracy: 0.7950\n",
      "Epoch 50/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4399 - accuracy: 0.8422 - val_loss: 0.5515 - val_accuracy: 0.7940\n",
      "Epoch 51/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4371 - accuracy: 0.8445 - val_loss: 0.5498 - val_accuracy: 0.7970\n",
      "Epoch 52/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4344 - accuracy: 0.8449 - val_loss: 0.5488 - val_accuracy: 0.7977\n",
      "Epoch 53/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4313 - accuracy: 0.8462 - val_loss: 0.5482 - val_accuracy: 0.7953\n",
      "Epoch 54/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4284 - accuracy: 0.8474 - val_loss: 0.5481 - val_accuracy: 0.7990\n",
      "Epoch 55/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.4259 - accuracy: 0.8483 - val_loss: 0.5510 - val_accuracy: 0.7963\n",
      "Epoch 56/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.4231 - accuracy: 0.8497 - val_loss: 0.5501 - val_accuracy: 0.7957\n",
      "Epoch 57/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.4203 - accuracy: 0.8497 - val_loss: 0.5471 - val_accuracy: 0.7993\n",
      "Epoch 58/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.4176 - accuracy: 0.8509 - val_loss: 0.5487 - val_accuracy: 0.7977\n",
      "Epoch 59/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.4152 - accuracy: 0.8522 - val_loss: 0.5475 - val_accuracy: 0.8013\n",
      "Epoch 60/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.4128 - accuracy: 0.8541 - val_loss: 0.5499 - val_accuracy: 0.7970\n",
      "Epoch 61/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.4106 - accuracy: 0.8546 - val_loss: 0.5526 - val_accuracy: 0.7983\n",
      "Epoch 62/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.4079 - accuracy: 0.8553 - val_loss: 0.5469 - val_accuracy: 0.8007\n",
      "Epoch 63/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4055 - accuracy: 0.8557 - val_loss: 0.5498 - val_accuracy: 0.7980\n",
      "Epoch 64/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4033 - accuracy: 0.8579 - val_loss: 0.5469 - val_accuracy: 0.8013\n",
      "Epoch 65/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.4012 - accuracy: 0.8585 - val_loss: 0.5485 - val_accuracy: 0.7997\n",
      "Epoch 66/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3987 - accuracy: 0.8593 - val_loss: 0.5498 - val_accuracy: 0.7990\n",
      "Epoch 67/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3965 - accuracy: 0.8587 - val_loss: 0.5474 - val_accuracy: 0.8007\n",
      "Epoch 68/120\n",
      "129/129 [==============================] - 1s 7ms/step - loss: 0.3946 - accuracy: 0.8603 - val_loss: 0.5490 - val_accuracy: 0.8023\n",
      "Epoch 69/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3924 - accuracy: 0.8611 - val_loss: 0.5476 - val_accuracy: 0.8023\n",
      "Epoch 70/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3903 - accuracy: 0.8616 - val_loss: 0.5480 - val_accuracy: 0.8033\n",
      "Epoch 71/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3882 - accuracy: 0.8626 - val_loss: 0.5494 - val_accuracy: 0.8000\n",
      "Epoch 72/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3864 - accuracy: 0.8638 - val_loss: 0.5507 - val_accuracy: 0.8010\n",
      "Epoch 73/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3842 - accuracy: 0.8640 - val_loss: 0.5487 - val_accuracy: 0.8027\n",
      "Epoch 74/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3823 - accuracy: 0.8648 - val_loss: 0.5499 - val_accuracy: 0.8057\n",
      "Epoch 75/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3799 - accuracy: 0.8659 - val_loss: 0.5578 - val_accuracy: 0.7967\n",
      "Epoch 76/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3782 - accuracy: 0.8656 - val_loss: 0.5511 - val_accuracy: 0.8027\n",
      "Epoch 77/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3766 - accuracy: 0.8673 - val_loss: 0.5497 - val_accuracy: 0.8047\n",
      "Epoch 78/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.3749 - accuracy: 0.8668 - val_loss: 0.5525 - val_accuracy: 0.8017\n",
      "Epoch 79/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.3729 - accuracy: 0.8684 - val_loss: 0.5516 - val_accuracy: 0.8030\n",
      "Epoch 80/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.3706 - accuracy: 0.8688 - val_loss: 0.5544 - val_accuracy: 0.8020\n",
      "Epoch 81/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.3692 - accuracy: 0.8696 - val_loss: 0.5521 - val_accuracy: 0.8037\n",
      "Epoch 82/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.3673 - accuracy: 0.8702 - val_loss: 0.5539 - val_accuracy: 0.8017\n",
      "Epoch 83/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.3660 - accuracy: 0.8710 - val_loss: 0.5519 - val_accuracy: 0.8050\n",
      "Epoch 84/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.3637 - accuracy: 0.8714 - val_loss: 0.5533 - val_accuracy: 0.8013\n",
      "Epoch 85/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3622 - accuracy: 0.8720 - val_loss: 0.5554 - val_accuracy: 0.8043\n",
      "Epoch 86/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3607 - accuracy: 0.8732 - val_loss: 0.5560 - val_accuracy: 0.8040\n",
      "Epoch 87/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3591 - accuracy: 0.8736 - val_loss: 0.5561 - val_accuracy: 0.8050\n",
      "Epoch 88/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3574 - accuracy: 0.8744 - val_loss: 0.5560 - val_accuracy: 0.8023\n",
      "Epoch 89/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3556 - accuracy: 0.8748 - val_loss: 0.5597 - val_accuracy: 0.8030\n",
      "Epoch 90/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3541 - accuracy: 0.8762 - val_loss: 0.5570 - val_accuracy: 0.8030\n",
      "Epoch 91/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3526 - accuracy: 0.8754 - val_loss: 0.5588 - val_accuracy: 0.8043\n",
      "Epoch 92/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3510 - accuracy: 0.8766 - val_loss: 0.5573 - val_accuracy: 0.8020\n",
      "Epoch 93/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3494 - accuracy: 0.8771 - val_loss: 0.5573 - val_accuracy: 0.8027\n",
      "Epoch 94/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3478 - accuracy: 0.8779 - val_loss: 0.5618 - val_accuracy: 0.8020\n",
      "Epoch 95/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3464 - accuracy: 0.8772 - val_loss: 0.5599 - val_accuracy: 0.8010\n",
      "Epoch 96/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3449 - accuracy: 0.8791 - val_loss: 0.5613 - val_accuracy: 0.8013\n",
      "Epoch 97/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3436 - accuracy: 0.8792 - val_loss: 0.5612 - val_accuracy: 0.8013\n",
      "Epoch 98/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3420 - accuracy: 0.8798 - val_loss: 0.5646 - val_accuracy: 0.8017\n",
      "Epoch 99/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3407 - accuracy: 0.8802 - val_loss: 0.5636 - val_accuracy: 0.8010\n",
      "Epoch 100/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.3391 - accuracy: 0.8808 - val_loss: 0.5680 - val_accuracy: 0.8030\n",
      "Epoch 101/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.3378 - accuracy: 0.8808 - val_loss: 0.5678 - val_accuracy: 0.8003\n",
      "Epoch 102/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.3368 - accuracy: 0.8812 - val_loss: 0.5683 - val_accuracy: 0.8000\n",
      "Epoch 103/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.3352 - accuracy: 0.8825 - val_loss: 0.5680 - val_accuracy: 0.8007\n",
      "Epoch 104/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.3339 - accuracy: 0.8817 - val_loss: 0.5669 - val_accuracy: 0.8007\n",
      "Epoch 105/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8826 - val_loss: 0.5708 - val_accuracy: 0.7973\n",
      "Epoch 106/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.3315 - accuracy: 0.8828 - val_loss: 0.5676 - val_accuracy: 0.7987\n",
      "Epoch 107/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.3292 - accuracy: 0.8845 - val_loss: 0.5730 - val_accuracy: 0.8023\n",
      "Epoch 108/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3285 - accuracy: 0.8835 - val_loss: 0.5689 - val_accuracy: 0.7993\n",
      "Epoch 109/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3268 - accuracy: 0.8851 - val_loss: 0.5772 - val_accuracy: 0.8013\n",
      "Epoch 110/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3257 - accuracy: 0.8852 - val_loss: 0.5751 - val_accuracy: 0.8027\n",
      "Epoch 111/120\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.3244 - accuracy: 0.8854 - val_loss: 0.5724 - val_accuracy: 0.7987\n",
      "Epoch 112/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3233 - accuracy: 0.8856 - val_loss: 0.5735 - val_accuracy: 0.7980\n",
      "Epoch 113/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3219 - accuracy: 0.8860 - val_loss: 0.5741 - val_accuracy: 0.7990\n",
      "Epoch 114/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3206 - accuracy: 0.8868 - val_loss: 0.5779 - val_accuracy: 0.8023\n",
      "Epoch 115/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3193 - accuracy: 0.8873 - val_loss: 0.5783 - val_accuracy: 0.8010\n",
      "Epoch 116/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3183 - accuracy: 0.8879 - val_loss: 0.5752 - val_accuracy: 0.7973\n",
      "Epoch 117/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3170 - accuracy: 0.8882 - val_loss: 0.5777 - val_accuracy: 0.7977\n",
      "Epoch 118/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3159 - accuracy: 0.8890 - val_loss: 0.5823 - val_accuracy: 0.8017\n",
      "Epoch 119/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3144 - accuracy: 0.8893 - val_loss: 0.5787 - val_accuracy: 0.7953\n",
      "Epoch 120/120\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.3134 - accuracy: 0.8893 - val_loss: 0.5844 - val_accuracy: 0.7967\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final, label_train_final, epochs=120, batch_size=256, validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, let's check the results returned from `model.evaluate()` to see how this model stacks up with the other techniques we've used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032/1032 [==============================] - 2s 1ms/step - loss: 0.3094 - accuracy: 0.8918\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6293 - accuracy: 0.7905\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3094480633735657, 0.8918485045433044]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output:  [0.31160746300942971, 0.89160606060606062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6293175220489502, 0.7904999852180481]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [0.56076071488857271, 0.8145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.1%. Our test set accuracy went up from ~75% to a staggering 81.45% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
